{
  "hash": "f33c36b48240bfebbed2b0c925fa2aca",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Assignment 04\nauthor:\n  - name: Norah Jones\n    affiliations:\n      - id: bu\n        name: Boston University\n        city: Boston\n        state: MA\nnumber-sections: true\ndate: '2024-11-21'\ndate-modified: today\ndate-format: long\nformat:\n  html:\n    theme: cerulean\n    toc: true\n    toc-depth: 2\n  docx: default\nexecute:\n  echo: true\n  eval: true\n  freeze: auto\n---\n\n\n\n# Data Preparartion\n## Load data and review\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nnp.random.seed(42)\n\npio.renderers.default = \"notebook+notebook_connected+vscode\"\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\n# Load Data\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"data/lightcast_job_postings.csv\")\n\n# Show Schema and Sample Data\n#print(\"---This is Diagnostic check, No need to print it in the final doc---\")\n\n#df.printSchema() # comment this line when rendering the submission\n#df.show(5)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 94:>                                                         (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n### Pick salary as target variable, state name, NAICS2_NAME name, remote type name, employment type name, city name, education levels name, min years experience , duration will be indepent variables for the analysis. \n\n### For min years experience and duration fill na with 0. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf = df.select(\"SALARY\",\"STATE_NAME\",\"NAICS2_NAME\", \"EDUCATION_LEVELS_NAME\", \"MIN_YEARS_EXPERIENCE\", \"DURATION\")\n#df.show()\ndf = df.na.fill({\"MIN_YEARS_EXPERIENCE\": 0, \"DURATION\":0})\n#df.show()\n```\n:::\n\n\n### Visualize the nas to understand the magnitude. Over 50% of salary is na, remove those values. \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndf_pd = df.toPandas()\n#df_pd.head(5)\n(df_pd.isna().sum() / len(df_pd)) * 100\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 95:>                                                         (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\nSALARY                   57.505035\nSTATE_NAME                0.060691\nNAICS2_NAME               0.060691\nEDUCATION_LEVELS_NAME     0.060691\nMIN_YEARS_EXPERIENCE      0.000000\nDURATION                  0.000000\ndtype: float64\n```\n:::\n:::\n\n\n\n\n### Using seaborn review a heat map of NA values. The independant variables are whole but salary is missing data.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nsns.heatmap(df_pd.isna(), cbar=False)\nplt.title(\"Missing Data Heatmap\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](assignment04-tluck15_files/figure-docx/cell-7-output-1.png){}\n:::\n:::\n\n\n### Drop all records where salary is NA.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndf = df.na.drop(subset=[\"SALARY\"])\ndf_pd = df.toPandas()\n\nsns.heatmap(df_pd.isna(), cbar=False)\nplt.title(\"Missing Data Heatmap\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 96:>                                                         (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](assignment04-tluck15_files/figure-docx/cell-8-output-2.png){}\n:::\n:::\n\n\n### The data is cleaned and is ready for modeling.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n(df_pd.isna().sum() / len(df_pd)) * 100\n```\n\n::: {.cell-output .cell-output-display execution_count=48}\n```\nSALARY                   0.0\nSTATE_NAME               0.0\nNAICS2_NAME              0.0\nEDUCATION_LEVELS_NAME    0.0\nMIN_YEARS_EXPERIENCE     0.0\nDURATION                 0.0\ndtype: float64\n```\n:::\n:::\n\n\n\n\n# Feature Engineering\n## First take the input variables and split into numeric and non numeric goups. State name, education levels, NAICS_NAME are all categoric variables. Min years experience and duration are numeric.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Suppose you have these columns\ncategorical_cols = [\"STATE_NAME\", \"EDUCATION_LEVELS_NAME\",\"NAICS2_NAME\"]\nnumeric_cols = [\"MIN_YEARS_EXPERIENCE\", \"DURATION\"]\n```\n:::\n\n\n### For the categorical columns, assign index values to each column and then one hot encode the columns as a vector.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nindexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\") for col in categorical_cols]\nencoders = [OneHotEncoder(inputCols=[f\"{col}_indexed\"], outputCols=[f\"{col}_encoded\"]) for col in categorical_cols]\n```\n:::\n\n\n### Next compile the one hot encoded columns with the numeric columns in a vector to be used in feature modeling.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nassembler_inputs = [f\"{col}_encoded\" for col in categorical_cols] + numeric_cols\nassembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n```\n:::\n\n\n### Store these data preparation steps as a pipeline for further use\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\npipeline = Pipeline(stages=indexers + encoders + [assembler])\n```\n:::\n\n\n### For polynomial square min years experience\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nfrom pyspark.sql.functions import col, pow\n\ndf_poly = df.withColumn(\"MIN_YEARS_EXPERIENCE_SQ\", pow(col(\"MIN_YEARS_EXPERIENCE\"), 2))\n```\n:::\n\n\n### Assemble vector using min years and min years experience for polynomial features.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nfrom pyspark.ml.feature import VectorAssembler\n\nassembler_poly = VectorAssembler(\n    inputCols=[\"MIN_YEARS_EXPERIENCE\", \"MIN_YEARS_EXPERIENCE_SQ\"],\n    outputCol=\"features_poly\"\n)\n\ndf_poly = assembler_poly.transform(df_poly)\n```\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ndf_poly.printSchema()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nroot\n |-- SALARY: integer (nullable = true)\n |-- STATE_NAME: string (nullable = true)\n |-- NAICS2_NAME: string (nullable = true)\n |-- EDUCATION_LEVELS_NAME: string (nullable = true)\n |-- MIN_YEARS_EXPERIENCE: integer (nullable = false)\n |-- DURATION: integer (nullable = false)\n |-- MIN_YEARS_EXPERIENCE_SQ: double (nullable = false)\n |-- features_poly: vector (nullable = true)\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n#df_poly.show()\n```\n:::\n\n\n### Now split the data for training and testing in a 70/30% split. \n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\ntrain_df, test_df = df.randomSplit([0.7, 0.3], seed=42)\n```\n:::\n\n\n### Use the previusly created pipeline the prepare the training and test data for use. \n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\npipeline_model = pipeline.fit(train_df)\ntrain_ready = pipeline_model.transform(train_df)\ntest_ready = pipeline_model.transform(test_df)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 97:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 103:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 109:>                                                        (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n### Parse target variable and features vector for modeling.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\ntrain_ready = train_ready[\"SALARY\",\"features\"]\n#train_ready.show()\n\ntest_ready = test_ready[\"SALARY\",\"features\"]\n```\n:::\n\n\n\n\n### Confirm schema for linear regression model.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ntrain_ready.printSchema()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nroot\n |-- SALARY: integer (nullable = true)\n |-- features: vector (nullable = true)\n\n```\n:::\n:::\n\n\n# Modeling\n## Build and fit model\n### Penalalize large coefficients and keep all features. Allow for intercept and standardize the data accross variables.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nlrm = LinearRegression(\n  featuresCol=\"features\",\n  labelCol=\"SALARY\",\n  predictionCol=\"prediction\",\n  maxIter=100,\n  regParam=0.1,\n  elasticNetParam=0.0,\n  fitIntercept=True,\n  standardization=True,\n)\n\nmodelLR = lrm.fit(train_ready)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 115:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 116:>                                                        (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n## Linear Regression Model Summary on Train and test data\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nsummary = modelLR.summary\n```\n:::\n\n\n### Issue existed with spillover so control with se, tvals, pvals. For features, loop through for only the number of coefficients so to avoid spillover.\n\n### Linear regression model performed well. All features had statistically significant p values, moderately large t values, reasonably equivalent coefficient magnitudes with respect to salary, which is reflected in the stndard error being significantly smaller than the respective coefficient.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nse = summary.coefficientStandardErrors[1:]\ntvals = summary.tValues[1:]\npvals = summary.pValues[1:]\n\ncoef_df = pd.DataFrame({\n    \"Feature\": [f\"feature_{i+1}\" for i in range(len(modelLR.coefficients))],\n    \"Coefficient\": modelLR.coefficients.toArray(),\n    \"StdError\": se,\n    \"tValue\": tvals,\n    \"pValue\": pvals\n})\n\ncoef_df.head(100)\n```\n\n::: {.cell-output .cell-output-display execution_count=65}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Coefficient</th>\n      <th>StdError</th>\n      <th>tValue</th>\n      <th>pValue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>feature_1</td>\n      <td>27160.771987</td>\n      <td>5586.524673</td>\n      <td>3.197485</td>\n      <td>1.388322e-03</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>feature_2</td>\n      <td>17862.826140</td>\n      <td>5604.708248</td>\n      <td>3.970201</td>\n      <td>7.204534e-05</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>feature_3</td>\n      <td>22251.815915</td>\n      <td>5628.321573</td>\n      <td>2.900919</td>\n      <td>3.724464e-03</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>feature_4</td>\n      <td>16327.306545</td>\n      <td>5639.799141</td>\n      <td>3.565083</td>\n      <td>3.645294e-04</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>feature_5</td>\n      <td>20106.349447</td>\n      <td>5650.125035</td>\n      <td>3.825928</td>\n      <td>1.306483e-04</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>feature_94</td>\n      <td>-13950.827368</td>\n      <td>9974.182772</td>\n      <td>-0.376460</td>\n      <td>7.065783e-01</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>feature_95</td>\n      <td>-3754.885150</td>\n      <td>10060.013381</td>\n      <td>0.278668</td>\n      <td>7.805023e-01</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>feature_96</td>\n      <td>2803.403880</td>\n      <td>73.807420</td>\n      <td>74.588198</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>feature_97</td>\n      <td>5505.162463</td>\n      <td>15.692949</td>\n      <td>-6.188578</td>\n      <td>6.180532e-10</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>feature_98</td>\n      <td>-97.117036</td>\n      <td>36792.595281</td>\n      <td>0.164635</td>\n      <td>8.692328e-01</td>\n    </tr>\n  </tbody>\n</table>\n<p>98 rows × 5 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Now validate and run the model using the test data\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\ntestmodelLR = lrm.fit(test_ready)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 117:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 118:>                                                        (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\ntestsummary = testmodelLR.summary\n\nt_values = summary.tValues\np_values = summary.pValues\n\nstats_df = pd.DataFrame({\n    \"t_value\": t_values,\n    \"p_value\": p_values\n})\n\ndesc_stats = stats_df.describe()\nprint(desc_stats)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         t_value    p_value\ncount  99.000000  99.000000\nmean    2.496936   0.176394\nstd     7.503150   0.265126\nmin    -6.188578   0.000000\n25%     0.898590   0.003787\n50%     2.039629   0.036848\n75%     2.887619   0.279912\nmax    74.588198   0.981592\n```\n:::\n:::\n\n\n### Issue existed with spillover so control with se, tvals, pvals. For features, loop through for only the number of coefficients so to avoid spillover.\n\n### The linear regression model performed okay on the test data. Accross the 99 features, the average p value was not significant, however the median value was significant at appx. .037 - the 75th percentile is not significant but the 25th is. THere were a few features that produced extreme results in all of the t value (~-6/74), p value (0/.98), and coefficient (a few spikes). R squared states ~35% of variation in salary can be explained by the features in the data. On average predictions are off by ~$35,834.\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nse = testsummary.coefficientStandardErrors[1:]\ntvals = testsummary.tValues[1:]\npvals = testsummary.pValues[1:]\n\ntestcoef_df = pd.DataFrame({\n    \"Feature\": [f\"feature_{i+1}\" for i in range(len(testmodelLR.coefficients))],\n    \"Coefficient\": testmodelLR.coefficients.toArray(),\n    \"StdError\": se,\n    \"tValue\": tvals,\n    \"pValue\": pvals\n})\n\nprint(\"R2:\", testsummary.r2)\nprint(\"RMSE:\", testsummary.rootMeanSquaredError)\n\n#coef_df.head(100)\n# Scale coefficients to t-values range\ntestcoef_df[\"Coefficient_log\"] = np.log(np.abs(testcoef_df[\"Coefficient\"]) + 1e-8) \n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,6))\n\n# x-axis = features\nx = testcoef_df[\"Feature\"]\n\n# plot each statistic as a line\nplt.plot(x, testcoef_df[\"Coefficient_log\"], marker='o', label=\"Coefficient Scaled\")\nplt.plot(x, testcoef_df[\"tValue\"], marker='s', label=\"t-value\")\nplt.plot(x, testcoef_df[\"pValue\"], marker='^', label=\"p-value\")\n\nplt.axhline(y=2, color='red', linestyle=':', label='t-value threshold')\n\nplt.xticks(rotation=45, ha='right')  # rotate feature names for readability\nplt.xlabel(\"Features\")\nplt.ylabel(\"Values\")\nplt.title(\"Model Summary: Coefficients, t-values, p-values\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR2: 0.35636023407578243\nRMSE: 35834.08039301742\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](assignment04-tluck15_files/figure-docx/cell-29-output-2.png){}\n:::\n:::\n\n\n# Polynomial Regression\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nfrom pyspark.ml.feature import PolynomialExpansion, VectorAssembler\n```\n:::\n\n\n## Combine numeric and non numeric columns.\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nassembler_poly = VectorAssembler(\n    inputCols=[\"MIN_YEARS_EXPERIENCE\", \"DURATION\"],\n    outputCol=\"numeric_features\"\n)\n\n#expand to polynomial\npoly_expansion = PolynomialExpansion(\n    degree=2, \n    inputCol=\"numeric_features\", \n    outputCol=\"poly_features\"\n)\n```\n:::\n\n\n### Use the pipeline created earlier to prepare the polynomial features.\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nassembler_final = VectorAssembler(\n    inputCols=[\"poly_features\"] + [f\"{col}_encoded\" for col in categorical_cols],\n    outputCol=\"features\"\n)\n\npoly_pipeline = Pipeline(stages=indexers + encoders + [assembler_poly, poly_expansion, assembler_final])\n```\n:::\n\n\n### Fit model and apply same parameters as the linear regression. ~36.8% of variation in salary can be explained by the feature variables. And on average the prediction was off by $36,025. \n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\npoly_model = poly_pipeline.fit(train_df)\ntrain_poly = poly_model.transform(train_df)\ntest_poly = poly_model.transform(test_df)\n\nlrm_poly = LinearRegression(\n    featuresCol=\"features\",\n    labelCol=\"SALARY\",\n    predictionCol=\"prediction\",\n    maxIter=100,\n    regParam=0.1,\n    elasticNetParam=0.0\n)\n\nmodel_poly = lrm_poly.fit(train_poly)\nsummary_poly = model_poly.summary\n\nprint(\"R2:\", summary_poly.r2)\nprint(\"RMSE:\", summary_poly.rootMeanSquaredError)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 119:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 125:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 131:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 137:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 138:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 139:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 140:>                                                        (0 + 1) / 1]\r\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nR2: 0.36831696100365496\nRMSE: 36025.51327867095\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r                                                                                \r\n```\n:::\n:::\n\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nse = summary_poly.coefficientStandardErrors[1:]\ntvals = summary_poly.tValues[1:]\npvals = summary_poly.pValues[1:]\n\ncoef_df = pd.DataFrame({\n    \"Feature\": [f\"feature_{i+1}\" for i in range(len(model_poly.coefficients))],\n    \"Coefficient\": model_poly.coefficients.toArray(),\n    \"StdError\": se,\n    \"tValue\": tvals,\n    \"pValue\": pvals\n})\n\ncoef_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=73}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Coefficient</th>\n      <th>StdError</th>\n      <th>tValue</th>\n      <th>pValue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>feature_1</td>\n      <td>4534.036373</td>\n      <td>16.938884</td>\n      <td>5.113944</td>\n      <td>3.182049e-07</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>feature_2</td>\n      <td>86.624507</td>\n      <td>49.042361</td>\n      <td>-8.224292</td>\n      <td>2.220446e-16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>feature_3</td>\n      <td>-403.338710</td>\n      <td>4.098090</td>\n      <td>0.271263</td>\n      <td>7.861914e-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>feature_4</td>\n      <td>1.111660</td>\n      <td>0.982271</td>\n      <td>6.941764</td>\n      <td>3.982370e-12</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>feature_5</td>\n      <td>6.818695</td>\n      <td>5553.886969</td>\n      <td>4.923227</td>\n      <td>8.575835e-07</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Overall the polynomial model performed worse than the linear regression model. All of the p value, t value, and coefficients experience high volatilitity and varying results. \n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\n# Scale coefficients with log\ncoef_df[\"Coefficient_log\"] = np.log(np.abs(coef_df[\"Coefficient\"]) + 1e-8) \n\nplt.figure(figsize=(12,6))\n\n# x-axis = features\nx = coef_df[\"Feature\"]\n\n# plot each statistic as a line\nplt.plot(x, coef_df[\"Coefficient_log\"], marker='o', label=\"Coefficient Scaled\")\nplt.plot(x, coef_df[\"tValue\"], marker='s', label=\"t-value\")\nplt.plot(x, coef_df[\"pValue\"], marker='^', label=\"p-value\")\n\nplt.axhline(y=2, color='red', linestyle=':', label='t-value threshold')\n\nplt.xticks(rotation=45, ha='right')  # rotate feature names for readability\nplt.xlabel(\"Features\")\nplt.ylabel(\"Values\")\nplt.title(\"Model Summary: Coefficients, t-values, p-values\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](assignment04-tluck15_files/figure-docx/cell-35-output-1.png){}\n:::\n:::\n\n\n# Random Forest\n## Setup model\n### 100 trees, 10 levels of depth to each branch, max of 18 bins for feature categorization.\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\nfrom pyspark.ml.regression import RandomForestRegressor\n\nrf = RandomForestRegressor(\n    labelCol=\"SALARY\",\n    featuresCol=\"features\",\n    numTrees=100,         \n    maxDepth=10,         \n    maxBins=18,        \n    seed=42\n)\n```\n:::\n\n\n### Like the other models, assemble the data and prepare it using the pipeline setup earlier in the project.\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\nrf_pipeline = Pipeline(stages=indexers + encoders + [assembler, rf])\n```\n:::\n\n\n### Train and then make predictions on test data.\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\nrf_model = rf_pipeline.fit(train_df)\npredictions = rf_model.transform(test_df)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 141:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 147:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 153:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 159:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 160:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 161:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 163:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 165:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 167:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 169:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 171:>                                                        (0 + 1) / 1]\r\r                                                                                \r25/10/17 14:17:42 WARN DAGScheduler: Broadcasting large task binary with size 1319.0 KiB\n\r[Stage 173:>                                                        (0 + 1) / 1]\r\r                                                                                \r25/10/17 14:17:44 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n\r[Stage 175:>                                                        (0 + 1) / 1]\r\r                                                                                \r25/10/17 14:17:47 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n\r[Stage 177:>                                                        (0 + 1) / 1]\r\r[Stage 178:>                                                        (0 + 1) / 1]\r\r                                                                                \r25/10/17 14:17:51 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n\r[Stage 179:>                                                        (0 + 1) / 1]\r\r[Stage 180:>                                                        (0 + 1) / 1]\r\r                                                                                \r25/10/17 14:17:56 WARN DAGScheduler: Broadcasting large task binary with size 7.4 MiB\n\r[Stage 181:>                                                        (0 + 1) / 1]\r\r[Stage 182:>                                                        (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n### Initial results suggest the random forest performed reasonably well. The r squared suggests ~43% of variation in salary can be explained by the features in the forest. Error is comparable to the other models as on average the prediction of salary was $25,175 off. \n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\nevaluator = RegressionEvaluator(\n    labelCol=\"SALARY\",\n    predictionCol=\"prediction\",\n    metricName=\"r2\"\n)\n\nr2 = evaluator.evaluate(predictions)\nrmse = RegressionEvaluator(\n    labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"rmse\"\n).evaluate(predictions)\n\nmae = RegressionEvaluator(\n    labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"mae\"\n).evaluate(predictions)\n\nprint(f\"R²: {r2:.3f}\")\nprint(f\"RMSE: {rmse:.3f}\")\nprint(f\"MAE: {mae:.3f}\")\n\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 183:>                                                        (0 + 1) / 1]\r\r[Stage 183:=========================================================(1 + 0) / 1]\r\r                                                                                \r\r[Stage 184:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 185:>                                                        (0 + 1) / 1]\r\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nR²: 0.429\nRMSE: 33744.372\nMAE: 25175.654\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r                                                                                \r\n```\n:::\n:::\n\n\n### Feature 97 was by far the most important feature.\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\nrf_stage = rf_model.stages[-1] \n\nimportances = rf_stage.featureImportances.toArray()\n\nfeat_imp = pd.DataFrame({\n    \"Feature\": [f\"feature_{i+1}\" for i in range(len(importances))],\n    \"Importance\": importances\n}).sort_values(by=\"Importance\", ascending=False)\n\nfeat_imp.head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=79}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>96</th>\n      <td>feature_97</td>\n      <td>0.521858</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>feature_55</td>\n      <td>0.083882</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>feature_77</td>\n      <td>0.069038</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>feature_81</td>\n      <td>0.045592</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>feature_98</td>\n      <td>0.031267</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>feature_56</td>\n      <td>0.029943</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>feature_84</td>\n      <td>0.025972</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>feature_53</td>\n      <td>0.016110</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>feature_78</td>\n      <td>0.013780</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>feature_52</td>\n      <td>0.013470</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(10,6))\nsns.barplot(data=feat_imp.head(15), y=\"Feature\", x=\"Importance\", palette=\"viridis\")\nplt.title(\"Top 15 Random Forest Feature Importances\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_3501/3261347940.py:5: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](assignment04-tluck15_files/figure-docx/cell-41-output-2.png){}\n:::\n:::\n\n\n# Model Comparisons\n## Summary\n### Overall the linear regression model was the model that produced the most consistent results on the test data. Generally speaking across all features the T-stat was consistently high (coeff/error), the p[ value was consistently at a level of significance (<.05), and this was representative of all variables, meaning all of the variables seemed to contribute reasonably to the outcome in terms of the coefficient magnitude, the t-stat and the p-value. The random foorest performed better in r squared (percent variation in salary explained by the features) but the rest of the supporting data was inconsinstent, especially the expnential impact of feature 97 on the random forest. The polynomial model also had inconsistencies, which make it less consistenly effective than the linear regression.\n\n### In total the linear regression accross the 99 features, the average p value was not significant, however the median value was significant at appx. .037 - the 75th percentile is not significant but the 25th is. There were a few features that produced extreme results in all of the t value (~-6/74), p value (0/.98), and coefficient (a few spikes). R squared states ~35% of variation in salary can be explained by the features in the data. On average predictions are off by ~$35,834.\n\n",
    "supporting": [
      "assignment04-tluck15_files/figure-docx"
    ],
    "filters": []
  }
}