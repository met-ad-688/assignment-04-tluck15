{
  "hash": "3ec9a90322e2fbce3416e7ef0c5dd9ef",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Assignment 04\nauthor:\n  - name: Norah Jones\n    affiliations:\n      - id: bu\n        name: Boston University\n        city: Boston\n        state: MA\nnumber-sections: true\ndate: '2024-11-21'\ndate-modified: today\ndate-format: long\nformat:\n  html:\n    theme: cerulean\n    toc: true\n    toc-depth: 2\n  docx: default\nexecute:\n  echo: true\n  eval: true\n  freeze: auto\n---\n\n\n\n# Data Preparartion\n## Load data and review\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nnp.random.seed(42)\n\npio.renderers.default = \"notebook+notebook_connected+vscode\"\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\n# Load Data\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"data/lightcast_job_postings.csv\")\n\n# Show Schema and Sample Data\n#print(\"---This is Diagnostic check, No need to print it in the final doc---\")\n\n#df.printSchema() # comment this line when rendering the submission\ndf.show(5)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 95:>                                                         (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------------------+-----------------+----------------------+----------+--------+---------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+--------+--------------------+-----------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+-------------+------+--------------+-----+--------------------+-----+----------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\n|                  ID|LAST_UPDATED_DATE|LAST_UPDATED_TIMESTAMP|DUPLICATES|  POSTED|  EXPIRED|DURATION|        SOURCE_TYPES|             SOURCES|                 URL|ACTIVE_URLS|ACTIVE_SOURCES_INFO|           TITLE_RAW|                BODY|MODELED_EXPIRED|MODELED_DURATION| COMPANY|        COMPANY_NAME|COMPANY_RAW|COMPANY_IS_STAFFING|EDUCATION_LEVELS|EDUCATION_LEVELS_NAME|MIN_EDULEVELS| MIN_EDULEVELS_NAME|MAX_EDULEVELS|MAX_EDULEVELS_NAME|EMPLOYMENT_TYPE|EMPLOYMENT_TYPE_NAME|MIN_YEARS_EXPERIENCE|MAX_YEARS_EXPERIENCE|IS_INTERNSHIP|SALARY|REMOTE_TYPE|REMOTE_TYPE_NAME|ORIGINAL_PAY_PERIOD|SALARY_TO|SALARY_FROM|            LOCATION|                CITY|    CITY_NAME|COUNTY|   COUNTY_NAME|  MSA|            MSA_NAME|STATE|STATE_NAME|COUNTY_OUTGOING|COUNTY_NAME_OUTGOING|COUNTY_INCOMING|COUNTY_NAME_INCOMING|MSA_OUTGOING|   MSA_NAME_OUTGOING|MSA_INCOMING|   MSA_NAME_INCOMING|NAICS2|         NAICS2_NAME|NAICS3|         NAICS3_NAME|NAICS4|         NAICS4_NAME|NAICS5|         NAICS5_NAME|NAICS6|         NAICS6_NAME|             TITLE|         TITLE_NAME|         TITLE_CLEAN|              SKILLS|         SKILLS_NAME|  SPECIALIZED_SKILLS|SPECIALIZED_SKILLS_NAME|      CERTIFICATIONS| CERTIFICATIONS_NAME|       COMMON_SKILLS|  COMMON_SKILLS_NAME|     SOFTWARE_SKILLS|SOFTWARE_SKILLS_NAME|      ONET|           ONET_NAME| ONET_2019|      ONET_2019_NAME|                CIP6|           CIP6_NAME|                CIP4|           CIP4_NAME|                CIP2|           CIP2_NAME|SOC_2021_2|     SOC_2021_2_NAME|SOC_2021_3|     SOC_2021_3_NAME|SOC_2021_4|SOC_2021_4_NAME|SOC_2021_5|SOC_2021_5_NAME|LOT_CAREER_AREA|LOT_CAREER_AREA_NAME|LOT_OCCUPATION| LOT_OCCUPATION_NAME|LOT_SPECIALIZED_OCCUPATION|LOT_SPECIALIZED_OCCUPATION_NAME|LOT_OCCUPATION_GROUP|LOT_OCCUPATION_GROUP_NAME|LOT_V6_SPECIALIZED_OCCUPATION|LOT_V6_SPECIALIZED_OCCUPATION_NAME|LOT_V6_OCCUPATION|LOT_V6_OCCUPATION_NAME|LOT_V6_OCCUPATION_GROUP|LOT_V6_OCCUPATION_GROUP_NAME|LOT_V6_CAREER_AREA|LOT_V6_CAREER_AREA_NAME|  SOC_2|          SOC_2_NAME|  SOC_3|          SOC_3_NAME|  SOC_4|     SOC_4_NAME|  SOC_5|     SOC_5_NAME|LIGHTCAST_SECTORS|LIGHTCAST_SECTORS_NAME|NAICS_2022_2|   NAICS_2022_2_NAME|NAICS_2022_3|   NAICS_2022_3_NAME|NAICS_2022_4|   NAICS_2022_4_NAME|NAICS_2022_5|   NAICS_2022_5_NAME|NAICS_2022_6|   NAICS_2022_6_NAME|\n+--------------------+-----------------+----------------------+----------+--------+---------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+--------+--------------------+-----------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+-------------+------+--------------+-----+--------------------+-----+----------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\n|1f57d95acf4dc67ed...|         9/6/2024|  2024-09-06 20:32:...|         0|6/2/2024| 6/8/2024|       6|   [\\n  \"Company\"\\n]|[\\n  \"brassring.c...|[\\n  \"https://sjo...|         []|               NULL|Enterprise Analys...|31-May-2024\\n\\nEn...|       6/8/2024|               6|  894731|          Murphy USA| Murphy USA|              false|       [\\n  2\\n]| [\\n  \"Bachelor's ...|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (> 32 h...|                   2|                   2|        false|  NULL|          0|          [None]|               NULL|     NULL|       NULL|{\\n  \"lat\": 33.20...|RWwgRG9yYWRvLCBBUg==|El Dorado, AR|  5139|     Union, AR|20980|       El Dorado, AR|    5|  Arkansas|           5139|           Union, AR|           5139|           Union, AR|       20980|       El Dorado, AR|       20980|       El Dorado, AR|    44|        Retail Trade|   441|Motor Vehicle and...|  4413|Automotive Parts,...| 44133|Automotive Parts ...|441330|Automotive Parts ...|ET29C073C03D1F86B4|Enterprise Analysts|enterprise analys...|[\\n  \"KS126DB6T06...|[\\n  \"Merchandisi...|[\\n  \"KS126DB6T06...|   [\\n  \"Merchandisi...|                  []|                  []|[\\n  \"KS126706DPF...|[\\n  \"Mathematics...|[\\n  \"KS440W865GC...|[\\n  \"SQL (Progra...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|[\\n  \"45.0601\",\\n...|[\\n  \"Economics, ...|[\\n  \"45.06\",\\n  ...|[\\n  \"Economics\",...|[\\n  \"45\",\\n  \"27...|[\\n  \"Social Scie...|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101011|           General ERP Analy...|                2310|     Business Intellig...|                     23101011|              General ERP Analy...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|        [\\n  7\\n]|  [\\n  \"Artificial ...|          44|        Retail Trade|         441|Motor Vehicle and...|        4413|Automotive Parts,...|       44133|Automotive Parts ...|      441330|Automotive Parts ...|\n|0cb072af26757b6c4...|         8/2/2024|  2024-08-02 17:08:...|         0|6/2/2024| 8/1/2024|    NULL| [\\n  \"Job Board\"\\n]| [\\n  \"maine.gov\"\\n]|[\\n  \"https://job...|         []|               NULL|Oracle Consultant...|Oracle Consultant...|       8/1/2024|            NULL|  133098|Smx Corporation L...|        SMX|               true|      [\\n  99\\n]| [\\n  \"No Educatio...|           99|No Education Listed|         NULL|              NULL|              1|Full-time (> 32 h...|                   3|                   3|        false|  NULL|          1|          Remote|               NULL|     NULL|       NULL|{\\n  \"lat\": 44.31...|    QXVndXN0YSwgTUU=|  Augusta, ME| 23011|  Kennebec, ME|12300|Augusta-Watervill...|   23|     Maine|          23011|        Kennebec, ME|          23011|        Kennebec, ME|       12300|Augusta-Watervill...|       12300|Augusta-Watervill...|    56|Administrative an...|   561|Administrative an...|  5613| Employment Services| 56132|Temporary Help Se...|561320|Temporary Help Se...|ET21DDA63780A7DC09| Oracle Consultants|oracle consultant...|[\\n  \"KS122626T55...|[\\n  \"Procurement...|[\\n  \"KS122626T55...|   [\\n  \"Procurement...|                  []|                  []|                  []|                  []|[\\n  \"BGSBF3F508F...|[\\n  \"Oracle Busi...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101012|           Oracle Consultant...|                2310|     Business Intellig...|                     23101012|              Oracle Consultant...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          56|Administrative an...|         561|Administrative an...|        5613| Employment Services|       56132|Temporary Help Se...|      561320|Temporary Help Se...|\n|85318b12b3331fa49...|         9/6/2024|  2024-09-06 20:32:...|         1|6/2/2024| 7/7/2024|      35| [\\n  \"Job Board\"\\n]|[\\n  \"dejobs.org\"\\n]|[\\n  \"https://dej...|         []|               NULL|        Data Analyst|Taking care of pe...|      6/10/2024|               8|39063746|            Sedgwick|   Sedgwick|              false|       [\\n  2\\n]| [\\n  \"Bachelor's ...|            2|  Bachelor's degree|         NULL|              NULL|              1|Full-time (> 32 h...|                   5|                NULL|        false|  NULL|          0|          [None]|               NULL|     NULL|       NULL|{\\n  \"lat\": 32.77...|    RGFsbGFzLCBUWA==|   Dallas, TX| 48113|    Dallas, TX|19100|Dallas-Fort Worth...|   48|     Texas|          48113|          Dallas, TX|          48113|          Dallas, TX|       19100|Dallas-Fort Worth...|       19100|Dallas-Fort Worth...|    52|Finance and Insur...|   524|Insurance Carrier...|  5242|Agencies, Brokera...| 52429|Other Insurance R...|524291|    Claims Adjusting|ET3037E0C947A02404|      Data Analysts|        data analyst|[\\n  \"KS1218W78FG...|[\\n  \"Management\"...|[\\n  \"ESF3939CE1F...|   [\\n  \"Exception R...|[\\n  \"KS683TN76T7...|[\\n  \"Security Cl...|[\\n  \"KS1218W78FG...|[\\n  \"Management\"...|[\\n  \"KS126HY6YLT...|[\\n  \"Microsoft O...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          52|Finance and Insur...|         524|Insurance Carrier...|        5242|Agencies, Brokera...|       52429|Other Insurance R...|      524291|    Claims Adjusting|\n|1b5c3941e54a1889e...|         9/6/2024|  2024-09-06 20:32:...|         1|6/2/2024|7/20/2024|      48| [\\n  \"Job Board\"\\n]|[\\n  \"disabledper...|[\\n  \"https://www...|         []|               NULL|Sr. Lead Data Mgm...|About this role:\\...|      6/12/2024|              10|37615159|         Wells Fargo|Wells Fargo|              false|      [\\n  99\\n]| [\\n  \"No Educatio...|           99|No Education Listed|         NULL|              NULL|              1|Full-time (> 32 h...|                   3|                NULL|        false|  NULL|          0|          [None]|               NULL|     NULL|       NULL|{\\n  \"lat\": 33.44...|    UGhvZW5peCwgQVo=|  Phoenix, AZ|  4013|  Maricopa, AZ|38060|Phoenix-Mesa-Chan...|    4|   Arizona|           4013|        Maricopa, AZ|           4013|        Maricopa, AZ|       38060|Phoenix-Mesa-Chan...|       38060|Phoenix-Mesa-Chan...|    52|Finance and Insur...|   522|Credit Intermedia...|  5221|Depository Credit...| 52211|  Commercial Banking|522110|  Commercial Banking|ET2114E0404BA30075|Management Analysts|sr lead data mgmt...|[\\n  \"KS123QX62QY...|[\\n  \"Exit Strate...|[\\n  \"KS123QX62QY...|   [\\n  \"Exit Strate...|                  []|                  []|[\\n  \"KS7G6NP6R6L...|[\\n  \"Reliability...|[\\n  \"KS4409D76NW...|[\\n  \"SAS (Softwa...|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231113|Data / Data Minin...|                  23111310|                   Data Analyst|                2311|     Data Analysis and...|                     23111310|                      Data Analyst|           231113|  Data / Data Minin...|                   2311|        Data Analysis and...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|        [\\n  6\\n]|  [\\n  \"Data Privac...|          52|Finance and Insur...|         522|Credit Intermedia...|        5221|Depository Credit...|       52211|  Commercial Banking|      522110|  Commercial Banking|\n|cb5ca25f02bdf25c1...|        6/19/2024|   2024-06-19 07:00:00|         0|6/2/2024|6/17/2024|      15|[\\n  \"FreeJobBoar...|[\\n  \"craigslist....|[\\n  \"https://mod...|         []|               NULL|Comisiones de $10...|Comisiones de $10...|      6/17/2024|              15|       0|        Unclassified|      LH/GM|              false|      [\\n  99\\n]| [\\n  \"No Educatio...|           99|No Education Listed|         NULL|              NULL|              3|Part-time / full-...|                NULL|                NULL|        false| 92500|          0|          [None]|               year|   150000|      35000|{\\n  \"lat\": 37.63...|    TW9kZXN0bywgQ0E=|  Modesto, CA|  6099|Stanislaus, CA|33700|         Modesto, CA|    6|California|           6099|      Stanislaus, CA|           6099|      Stanislaus, CA|       33700|         Modesto, CA|       33700|         Modesto, CA|    99|Unclassified Indu...|   999|Unclassified Indu...|  9999|Unclassified Indu...| 99999|Unclassified Indu...|999999|Unclassified Indu...|ET0000000000000000|       Unclassified|comisiones de por...|                  []|                  []|                  []|                     []|                  []|                  []|                  []|                  []|                  []|                  []|15-2051.01|Business Intellig...|15-2051.01|Business Intellig...|                  []|                  []|                  []|                  []|                  []|                  []|   15-0000|Computer and Math...|   15-2000|Mathematical Scie...|   15-2050|Data Scientists|   15-2051|Data Scientists|             23|Information Techn...|        231010|Business Intellig...|                  23101012|           Oracle Consultant...|                2310|     Business Intellig...|                     23101012|              Oracle Consultant...|           231010|  Business Intellig...|                   2310|        Business Intellig...|                23|   Information Techn...|15-0000|Computer and Math...|15-2000|Mathematical Scie...|15-2050|Data Scientists|15-2051|Data Scientists|             NULL|                  NULL|          99|Unclassified Indu...|         999|Unclassified Indu...|        9999|Unclassified Indu...|       99999|Unclassified Indu...|      999999|Unclassified Indu...|\n+--------------------+-----------------+----------------------+----------+--------+---------+--------+--------------------+--------------------+--------------------+-----------+-------------------+--------------------+--------------------+---------------+----------------+--------+--------------------+-----------+-------------------+----------------+---------------------+-------------+-------------------+-------------+------------------+---------------+--------------------+--------------------+--------------------+-------------+------+-----------+----------------+-------------------+---------+-----------+--------------------+--------------------+-------------+------+--------------+-----+--------------------+-----+----------+---------------+--------------------+---------------+--------------------+------------+--------------------+------------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------+--------------------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+----------+--------------------+----------+---------------+----------+---------------+---------------+--------------------+--------------+--------------------+--------------------------+-------------------------------+--------------------+-------------------------+-----------------------------+----------------------------------+-----------------+----------------------+-----------------------+----------------------------+------------------+-----------------------+-------+--------------------+-------+--------------------+-------+---------------+-------+---------------+-----------------+----------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+------------+--------------------+\nonly showing top 5 rows\n```\n:::\n:::\n\n\n### Pick salary as target variable, state name, NAICS2_NAME name, remote type name, employment type name, city name, education levels name, min years experience , duration will be indepent variables for the analysis. \n\n### For min years experience and duration fill na with 0. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf = df.select(\"SALARY\",\"STATE_NAME\",\"NAICS2_NAME\", \"EDUCATION_LEVELS_NAME\", \"MIN_YEARS_EXPERIENCE\", \"DURATION\")\n#df.show()\ndf = df.na.fill({\"MIN_YEARS_EXPERIENCE\": 0, \"DURATION\":0})\n#df.show()\n```\n:::\n\n\n### Visualize the nas to understand the magnitude. Over 50% of salary is na, remove those values. \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndf_pd = df.toPandas()\n#df_pd.head(5)\n(df_pd.isna().sum() / len(df_pd)) * 100\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 97:>                                                         (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\nSALARY                   57.505035\nSTATE_NAME                0.060691\nNAICS2_NAME               0.060691\nEDUCATION_LEVELS_NAME     0.060691\nMIN_YEARS_EXPERIENCE      0.000000\nDURATION                  0.000000\ndtype: float64\n```\n:::\n:::\n\n\n\n\n### Using seaborn review a heat map of NA values. The independant variables are whole but salary is missing data.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nsns.heatmap(df_pd.isna(), cbar=False)\nplt.title(\"Missing Data Heatmap\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](assignment04-tluck15_files/figure-docx/cell-7-output-1.png){}\n:::\n:::\n\n\n### Drop all records where salary is NA.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndf = df.na.drop(subset=[\"SALARY\"])\ndf_pd = df.toPandas()\n\nsns.heatmap(df_pd.isna(), cbar=False)\nplt.title(\"Missing Data Heatmap\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 98:>                                                         (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](assignment04-tluck15_files/figure-docx/cell-8-output-2.png){}\n:::\n:::\n\n\n### The data is cleaned and is ready for modeling.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n(df_pd.isna().sum() / len(df_pd)) * 100\n```\n\n::: {.cell-output .cell-output-display execution_count=48}\n```\nSALARY                   0.0\nSTATE_NAME               0.0\nNAICS2_NAME              0.0\nEDUCATION_LEVELS_NAME    0.0\nMIN_YEARS_EXPERIENCE     0.0\nDURATION                 0.0\ndtype: float64\n```\n:::\n:::\n\n\n\n\n# Feature Engineering\n## First take the input variables and split into numeric and non numeric goups. State name, education levels, NAICS_NAME are all categoric variables. Min years experience and duration are numeric.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Suppose you have these columns\ncategorical_cols = [\"STATE_NAME\", \"EDUCATION_LEVELS_NAME\",\"NAICS2_NAME\"]\nnumeric_cols = [\"MIN_YEARS_EXPERIENCE\", \"DURATION\"]\n```\n:::\n\n\n### For the categorical columns, assign index values to each column and then one hot encode the columns as a vector.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nindexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\") for col in categorical_cols]\nencoders = [OneHotEncoder(inputCols=[f\"{col}_indexed\"], outputCols=[f\"{col}_encoded\"]) for col in categorical_cols]\n```\n:::\n\n\n### Next compile the one hot encoded columns with the numeric columns in a vector to be used in feature modeling.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nassembler_inputs = [f\"{col}_encoded\" for col in categorical_cols] + numeric_cols\nassembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n```\n:::\n\n\n### Store these data preparation steps as a pipeline for further use\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\npipeline = Pipeline(stages=indexers + encoders + [assembler])\n```\n:::\n\n\n### For polynomial square min years experience\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nfrom pyspark.sql.functions import col, pow\n\ndf_poly = df.withColumn(\"MIN_YEARS_EXPERIENCE_SQ\", pow(col(\"MIN_YEARS_EXPERIENCE\"), 2))\n```\n:::\n\n\n### Assemble vector using min years and min years experience for polynomial features.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nfrom pyspark.ml.feature import VectorAssembler\n\nassembler_poly = VectorAssembler(\n    inputCols=[\"MIN_YEARS_EXPERIENCE\", \"MIN_YEARS_EXPERIENCE_SQ\"],\n    outputCol=\"features_poly\"\n)\n\ndf_poly = assembler_poly.transform(df_poly)\n```\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ndf_poly.printSchema()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nroot\n |-- SALARY: integer (nullable = true)\n |-- STATE_NAME: string (nullable = true)\n |-- NAICS2_NAME: string (nullable = true)\n |-- EDUCATION_LEVELS_NAME: string (nullable = true)\n |-- MIN_YEARS_EXPERIENCE: integer (nullable = false)\n |-- DURATION: integer (nullable = false)\n |-- MIN_YEARS_EXPERIENCE_SQ: double (nullable = false)\n |-- features_poly: vector (nullable = true)\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n#df_poly.show()\n```\n:::\n\n\n### Now split the data for training and testing in a 70/30% split. \n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\ntrain_df, test_df = df.randomSplit([0.7, 0.3], seed=42)\n```\n:::\n\n\n### Use the previusly created pipeline the prepare the training and test data for use. \n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\npipeline_model = pipeline.fit(train_df)\ntrain_ready = pipeline_model.transform(train_df)\ntest_ready = pipeline_model.transform(test_df)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 99:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 105:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 111:>                                                        (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n### Parse target variable and features vector for modeling.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\ntrain_ready = train_ready[\"SALARY\",\"features\"]\n#train_ready.show()\n\ntest_ready = test_ready[\"SALARY\",\"features\"]\n```\n:::\n\n\n\n\n### Confirm schema for linear regression model.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ntrain_ready.printSchema()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nroot\n |-- SALARY: integer (nullable = true)\n |-- features: vector (nullable = true)\n\n```\n:::\n:::\n\n\n# Modeling\n## Build and fit model\n### Penalalize large coefficients and keep all features. Allow for intercept and standardize the data accross variables.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nlrm = LinearRegression(\n  featuresCol=\"features\",\n  labelCol=\"SALARY\",\n  predictionCol=\"prediction\",\n  maxIter=100,\n  regParam=0.1,\n  elasticNetParam=0.0,\n  fitIntercept=True,\n  standardization=True,\n)\n\nmodelLR = lrm.fit(train_ready)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 117:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 118:>                                                        (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n## Linear Regression Model Summary on Train and test data\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nsummary = modelLR.summary\n```\n:::\n\n\n### Issue existed with spillover so control with se, tvals, pvals. For features, loop through for only the number of coefficients so to avoid spillover.\n\n### Linear regression model performed well. All features had statistically significant p values, moderately large t values, reasonably equivalent coefficient magnitudes with respect to salary, which is reflected in the stndard error being significantly smaller than the respective coefficient.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nse = summary.coefficientStandardErrors[1:]\ntvals = summary.tValues[1:]\npvals = summary.pValues[1:]\n\ncoef_df = pd.DataFrame({\n    \"Feature\": [f\"feature_{i+1}\" for i in range(len(modelLR.coefficients))],\n    \"Coefficient\": modelLR.coefficients.toArray(),\n    \"StdError\": se,\n    \"tValue\": tvals,\n    \"pValue\": pvals\n})\n\ncoef_df.head(100)\n```\n\n::: {.cell-output .cell-output-display execution_count=65}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Coefficient</th>\n      <th>StdError</th>\n      <th>tValue</th>\n      <th>pValue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>feature_1</td>\n      <td>27160.771987</td>\n      <td>5586.524673</td>\n      <td>3.197485</td>\n      <td>1.388322e-03</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>feature_2</td>\n      <td>17862.826140</td>\n      <td>5604.708248</td>\n      <td>3.970201</td>\n      <td>7.204534e-05</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>feature_3</td>\n      <td>22251.815915</td>\n      <td>5628.321573</td>\n      <td>2.900919</td>\n      <td>3.724464e-03</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>feature_4</td>\n      <td>16327.306545</td>\n      <td>5639.799141</td>\n      <td>3.565083</td>\n      <td>3.645294e-04</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>feature_5</td>\n      <td>20106.349447</td>\n      <td>5650.125035</td>\n      <td>3.825928</td>\n      <td>1.306483e-04</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>feature_94</td>\n      <td>-13950.827368</td>\n      <td>9974.182772</td>\n      <td>-0.376460</td>\n      <td>7.065783e-01</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>feature_95</td>\n      <td>-3754.885150</td>\n      <td>10060.013381</td>\n      <td>0.278668</td>\n      <td>7.805023e-01</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>feature_96</td>\n      <td>2803.403880</td>\n      <td>73.807420</td>\n      <td>74.588198</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>feature_97</td>\n      <td>5505.162463</td>\n      <td>15.692949</td>\n      <td>-6.188578</td>\n      <td>6.180532e-10</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>feature_98</td>\n      <td>-97.117036</td>\n      <td>36792.595281</td>\n      <td>0.164635</td>\n      <td>8.692328e-01</td>\n    </tr>\n  </tbody>\n</table>\n<p>98 rows × 5 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Now validate and run the model using the test data\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\ntestmodelLR = lrm.fit(test_ready)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 119:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 120:>                                                        (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\ntestsummary = testmodelLR.summary\n\nt_values = summary.tValues\np_values = summary.pValues\n\nstats_df = pd.DataFrame({\n    \"t_value\": t_values,\n    \"p_value\": p_values\n})\n\ndesc_stats = stats_df.describe()\nprint(desc_stats)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         t_value    p_value\ncount  99.000000  99.000000\nmean    2.496936   0.176394\nstd     7.503150   0.265126\nmin    -6.188578   0.000000\n25%     0.898590   0.003787\n50%     2.039629   0.036848\n75%     2.887619   0.279912\nmax    74.588198   0.981592\n```\n:::\n:::\n\n\n### Issue existed with spillover so control with se, tvals, pvals. For features, loop through for only the number of coefficients so to avoid spillover.\n\n### The linear regression model performed okay on the test data. Accross the 99 features, the average p value was not significant, however the median value was significant at appx. .037 - the 75th percentile is not significant but the 25th is. THere were a few features that produced extreme results in all of the t value (~-6/74), p value (0/.98), and coefficient (a few spikes). R squared states ~35% of variation in salary can be explained by the features in the data. On average predictions are off by ~$35,834.\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nse = testsummary.coefficientStandardErrors[1:]\ntvals = testsummary.tValues[1:]\npvals = testsummary.pValues[1:]\n\ntestcoef_df = pd.DataFrame({\n    \"Feature\": [f\"feature_{i+1}\" for i in range(len(testmodelLR.coefficients))],\n    \"Coefficient\": testmodelLR.coefficients.toArray(),\n    \"StdError\": se,\n    \"tValue\": tvals,\n    \"pValue\": pvals\n})\n\nprint(\"R2:\", testsummary.r2)\nprint(\"RMSE:\", testsummary.rootMeanSquaredError)\n\n#coef_df.head(100)\n# Scale coefficients to t-values range\ntestcoef_df[\"Coefficient_log\"] = np.log(np.abs(testcoef_df[\"Coefficient\"]) + 1e-8) \n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,6))\n\n# x-axis = features\nx = testcoef_df[\"Feature\"]\n\n# plot each statistic as a line\nplt.plot(x, testcoef_df[\"Coefficient_log\"], marker='o', label=\"Coefficient Scaled\")\nplt.plot(x, testcoef_df[\"tValue\"], marker='s', label=\"t-value\")\nplt.plot(x, testcoef_df[\"pValue\"], marker='^', label=\"p-value\")\n\nplt.axhline(y=2, color='red', linestyle=':', label='t-value threshold')\n\nplt.xticks(rotation=45, ha='right')  # rotate feature names for readability\nplt.xlabel(\"Features\")\nplt.ylabel(\"Values\")\nplt.title(\"Model Summary: Coefficients, t-values, p-values\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR2: 0.35636023407578243\nRMSE: 35834.08039301742\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](assignment04-tluck15_files/figure-docx/cell-29-output-2.png){}\n:::\n:::\n\n\n# Polynomial Regression\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nfrom pyspark.ml.feature import PolynomialExpansion, VectorAssembler\n```\n:::\n\n\n## Combine numeric and non numeric columns.\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nassembler_poly = VectorAssembler(\n    inputCols=[\"MIN_YEARS_EXPERIENCE\", \"DURATION\"],\n    outputCol=\"numeric_features\"\n)\n\n#expand to polynomial\npoly_expansion = PolynomialExpansion(\n    degree=2, \n    inputCol=\"numeric_features\", \n    outputCol=\"poly_features\"\n)\n```\n:::\n\n\n### Use the pipeline created earlier to prepare the polynomial features.\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nassembler_final = VectorAssembler(\n    inputCols=[\"poly_features\"] + [f\"{col}_encoded\" for col in categorical_cols],\n    outputCol=\"features\"\n)\n\npoly_pipeline = Pipeline(stages=indexers + encoders + [assembler_poly, poly_expansion, assembler_final])\n```\n:::\n\n\n### Fit model and apply same parameters as the linear regression. ~36.8% of variation in salary can be explained by the feature variables. And on average the prediction was off by $36,025. \n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\npoly_model = poly_pipeline.fit(train_df)\ntrain_poly = poly_model.transform(train_df)\ntest_poly = poly_model.transform(test_df)\n\nlrm_poly = LinearRegression(\n    featuresCol=\"features\",\n    labelCol=\"SALARY\",\n    predictionCol=\"prediction\",\n    maxIter=100,\n    regParam=0.1,\n    elasticNetParam=0.0\n)\n\nmodel_poly = lrm_poly.fit(train_poly)\nsummary_poly = model_poly.summary\n\nprint(\"R2:\", summary_poly.r2)\nprint(\"RMSE:\", summary_poly.rootMeanSquaredError)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 121:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 127:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 133:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 139:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 140:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 141:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 142:>                                                        (0 + 1) / 1]\r\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nR2: 0.36831696100365496\nRMSE: 36025.51327867095\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r                                                                                \r\n```\n:::\n:::\n\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nse = summary_poly.coefficientStandardErrors[1:]\ntvals = summary_poly.tValues[1:]\npvals = summary_poly.pValues[1:]\n\ncoef_df = pd.DataFrame({\n    \"Feature\": [f\"feature_{i+1}\" for i in range(len(model_poly.coefficients))],\n    \"Coefficient\": model_poly.coefficients.toArray(),\n    \"StdError\": se,\n    \"tValue\": tvals,\n    \"pValue\": pvals\n})\n\ncoef_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=73}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Coefficient</th>\n      <th>StdError</th>\n      <th>tValue</th>\n      <th>pValue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>feature_1</td>\n      <td>4534.036373</td>\n      <td>16.938884</td>\n      <td>5.113944</td>\n      <td>3.182049e-07</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>feature_2</td>\n      <td>86.624507</td>\n      <td>49.042361</td>\n      <td>-8.224292</td>\n      <td>2.220446e-16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>feature_3</td>\n      <td>-403.338710</td>\n      <td>4.098090</td>\n      <td>0.271263</td>\n      <td>7.861914e-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>feature_4</td>\n      <td>1.111660</td>\n      <td>0.982271</td>\n      <td>6.941764</td>\n      <td>3.982370e-12</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>feature_5</td>\n      <td>6.818695</td>\n      <td>5553.886969</td>\n      <td>4.923227</td>\n      <td>8.575835e-07</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Overall the polynomial model performed worse than the linear regression model. All of the p value, t value, and coefficients experience high volatilitity  and varying results. \n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\n# Scale coefficients with log\ncoef_df[\"Coefficient_log\"] = np.log(np.abs(coef_df[\"Coefficient\"]) + 1e-8) \n\nplt.figure(figsize=(12,6))\n\n# x-axis = features\nx = coef_df[\"Feature\"]\n\n# plot each statistic as a line\nplt.plot(x, coef_df[\"Coefficient_log\"], marker='o', label=\"Coefficient Scaled\")\nplt.plot(x, coef_df[\"tValue\"], marker='s', label=\"t-value\")\nplt.plot(x, coef_df[\"pValue\"], marker='^', label=\"p-value\")\n\nplt.axhline(y=2, color='red', linestyle=':', label='t-value threshold')\n\nplt.xticks(rotation=45, ha='right')  # rotate feature names for readability\nplt.xlabel(\"Features\")\nplt.ylabel(\"Values\")\nplt.title(\"Model Summary: Coefficients, t-values, p-values\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](assignment04-tluck15_files/figure-docx/cell-35-output-1.png){}\n:::\n:::\n\n\n# Random Forest\n## Setup model\n### 100 trees, 10 levels of depth to each branch, max of 18 bins for feature categorization.\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\nfrom pyspark.ml.regression import RandomForestRegressor\n\nrf = RandomForestRegressor(\n    labelCol=\"SALARY\",\n    featuresCol=\"features\",\n    numTrees=100,           # number of trees (more trees = more stability)\n    maxDepth=10,            # depth of each tree\n    maxBins=18,             # controls how continuous features are binned\n    seed=42\n)\n```\n:::\n\n\n### Like the other models, assemble the data and prepare it using the pipeline setup earlier in the project.\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\nrf_pipeline = Pipeline(stages=indexers + encoders + [assembler, rf])\n```\n:::\n\n\n### Train and then make predictions on test data. [stopping point]\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\nrf_model = rf_pipeline.fit(train_df)\npredictions = rf_model.transform(test_df)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 143:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 149:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 155:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 161:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 162:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 163:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 165:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 167:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 169:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 171:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 173:>                                                        (0 + 1) / 1]\r\r                                                                                \r25/10/17 03:39:51 WARN DAGScheduler: Broadcasting large task binary with size 1319.0 KiB\n\r[Stage 175:>                                                        (0 + 1) / 1]\r\r                                                                                \r25/10/17 03:39:53 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n\r[Stage 177:>                                                        (0 + 1) / 1]\r\r                                                                                \r25/10/17 03:39:56 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n\r[Stage 179:>                                                        (0 + 1) / 1]\r\r                                                                                \r25/10/17 03:39:59 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n\r[Stage 181:>                                                        (0 + 1) / 1]\r\r[Stage 182:>                                                        (0 + 1) / 1]\r\r                                                                                \r25/10/17 03:40:05 WARN DAGScheduler: Broadcasting large task binary with size 7.4 MiB\n\r[Stage 183:>                                                        (0 + 1) / 1]\r\r[Stage 184:>                                                        (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\nevaluator = RegressionEvaluator(\n    labelCol=\"SALARY\",\n    predictionCol=\"prediction\",\n    metricName=\"r2\"\n)\n\nr2 = evaluator.evaluate(predictions)\nrmse = RegressionEvaluator(\n    labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"rmse\"\n).evaluate(predictions)\n\nmae = RegressionEvaluator(\n    labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"mae\"\n).evaluate(predictions)\n\nprint(f\"R²: {r2:.3f}\")\nprint(f\"RMSE: {rmse:.3f}\")\nprint(f\"MAE: {mae:.3f}\")\n\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 185:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 186:>                                                        (0 + 1) / 1]\r\r                                                                                \r\r[Stage 187:>                                                        (0 + 1) / 1]\r\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nR²: 0.429\nRMSE: 33744.372\nMAE: 25175.654\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r                                                                                \r\n```\n:::\n:::\n\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\nrf_stage = rf_model.stages[-1] \n\nimportances = rf_stage.featureImportances.toArray()\n\nfeat_imp = pd.DataFrame({\n    \"Feature\": [f\"feature_{i+1}\" for i in range(len(importances))],\n    \"Importance\": importances\n}).sort_values(by=\"Importance\", ascending=False)\n\nfeat_imp.head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=79}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>96</th>\n      <td>feature_97</td>\n      <td>0.521858</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>feature_55</td>\n      <td>0.083882</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>feature_77</td>\n      <td>0.069038</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>feature_81</td>\n      <td>0.045592</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>feature_98</td>\n      <td>0.031267</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>feature_56</td>\n      <td>0.029943</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>feature_84</td>\n      <td>0.025972</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>feature_53</td>\n      <td>0.016110</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>feature_78</td>\n      <td>0.013780</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>feature_52</td>\n      <td>0.013470</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(10,6))\nsns.barplot(data=feat_imp.head(15), y=\"Feature\", x=\"Importance\", palette=\"viridis\")\nplt.title(\"Top 15 Random Forest Feature Importances\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_8037/3261347940.py:5: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](assignment04-tluck15_files/figure-docx/cell-41-output-2.png){}\n:::\n:::\n\n\n# Model Comparisons\n\n",
    "supporting": [
      "assignment04-tluck15_files/figure-docx"
    ],
    "filters": []
  }
}