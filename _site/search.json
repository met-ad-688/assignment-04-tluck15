[
  {
    "objectID": "assignment04-tluck15.html",
    "href": "assignment04-tluck15.html",
    "title": "Assignment 04",
    "section": "",
    "text": "np.random.seed(42)\n\npio.renderers.default = \"notebook+notebook_connected+vscode\"\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\n# Load Data\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"data/lightcast_job_postings.csv\")\n\n# Show Schema and Sample Data\n#print(\"---This is Diagnostic check, No need to print it in the final doc---\")\n\n#df.printSchema() # comment this line when rendering the submission\n#df.show(5)\n\nWARNING: Using incubator modules: jdk.incubator.vector\nUsing Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/10/17 14:12:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n25/10/17 14:12:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n[Stage 1:&gt;                                                          (0 + 1) / 1]                                                                                \n\n\n\n\n\n\n\n\ndf = df.select(\"SALARY\",\"STATE_NAME\",\"NAICS2_NAME\", \"EDUCATION_LEVELS_NAME\", \"MIN_YEARS_EXPERIENCE\", \"DURATION\")\n#df.show()\ndf = df.na.fill({\"MIN_YEARS_EXPERIENCE\": 0, \"DURATION\":0})\n#df.show()\n\n\n\n\n\ndf_pd = df.toPandas()\n#df_pd.head(5)\n(df_pd.isna().sum() / len(df_pd)) * 100\n\n[Stage 2:&gt;                                                          (0 + 1) / 1]                                                                                \n\n\nSALARY                   57.505035\nSTATE_NAME                0.060691\nNAICS2_NAME               0.060691\nEDUCATION_LEVELS_NAME     0.060691\nMIN_YEARS_EXPERIENCE      0.000000\nDURATION                  0.000000\ndtype: float64\n\n\n\n\n\n\nsns.heatmap(df_pd.isna(), cbar=False)\nplt.title(\"Missing Data Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\ndf = df.na.drop(subset=[\"SALARY\"])\ndf_pd = df.toPandas()\n\nsns.heatmap(df_pd.isna(), cbar=False)\nplt.title(\"Missing Data Heatmap\")\nplt.show()\n\n[Stage 3:&gt;                                                          (0 + 1) / 1]                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n(df_pd.isna().sum() / len(df_pd)) * 100\n\nSALARY                   0.0\nSTATE_NAME               0.0\nNAICS2_NAME              0.0\nEDUCATION_LEVELS_NAME    0.0\nMIN_YEARS_EXPERIENCE     0.0\nDURATION                 0.0\ndtype: float64"
  },
  {
    "objectID": "assignment04-tluck15.html#load-data-and-review",
    "href": "assignment04-tluck15.html#load-data-and-review",
    "title": "Assignment 04",
    "section": "",
    "text": "np.random.seed(42)\n\npio.renderers.default = \"notebook+notebook_connected+vscode\"\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n\n# Load Data\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"data/lightcast_job_postings.csv\")\n\n# Show Schema and Sample Data\n#print(\"---This is Diagnostic check, No need to print it in the final doc---\")\n\n#df.printSchema() # comment this line when rendering the submission\n#df.show(5)\n\nWARNING: Using incubator modules: jdk.incubator.vector\nUsing Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/10/17 14:12:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n25/10/17 14:12:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n[Stage 1:&gt;                                                          (0 + 1) / 1]                                                                                \n\n\n\n\n\n\n\n\ndf = df.select(\"SALARY\",\"STATE_NAME\",\"NAICS2_NAME\", \"EDUCATION_LEVELS_NAME\", \"MIN_YEARS_EXPERIENCE\", \"DURATION\")\n#df.show()\ndf = df.na.fill({\"MIN_YEARS_EXPERIENCE\": 0, \"DURATION\":0})\n#df.show()\n\n\n\n\n\ndf_pd = df.toPandas()\n#df_pd.head(5)\n(df_pd.isna().sum() / len(df_pd)) * 100\n\n[Stage 2:&gt;                                                          (0 + 1) / 1]                                                                                \n\n\nSALARY                   57.505035\nSTATE_NAME                0.060691\nNAICS2_NAME               0.060691\nEDUCATION_LEVELS_NAME     0.060691\nMIN_YEARS_EXPERIENCE      0.000000\nDURATION                  0.000000\ndtype: float64\n\n\n\n\n\n\nsns.heatmap(df_pd.isna(), cbar=False)\nplt.title(\"Missing Data Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\ndf = df.na.drop(subset=[\"SALARY\"])\ndf_pd = df.toPandas()\n\nsns.heatmap(df_pd.isna(), cbar=False)\nplt.title(\"Missing Data Heatmap\")\nplt.show()\n\n[Stage 3:&gt;                                                          (0 + 1) / 1]                                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n(df_pd.isna().sum() / len(df_pd)) * 100\n\nSALARY                   0.0\nSTATE_NAME               0.0\nNAICS2_NAME              0.0\nEDUCATION_LEVELS_NAME    0.0\nMIN_YEARS_EXPERIENCE     0.0\nDURATION                 0.0\ndtype: float64"
  },
  {
    "objectID": "assignment04-tluck15.html#first-take-the-input-variables-and-split-into-numeric-and-non-numeric-goups.-state-name-education-levels-naics_name-are-all-categoric-variables.-min-years-experience-and-duration-are-numeric.",
    "href": "assignment04-tluck15.html#first-take-the-input-variables-and-split-into-numeric-and-non-numeric-goups.-state-name-education-levels-naics_name-are-all-categoric-variables.-min-years-experience-and-duration-are-numeric.",
    "title": "Assignment 04",
    "section": "2.1 First take the input variables and split into numeric and non numeric goups. State name, education levels, NAICS_NAME are all categoric variables. Min years experience and duration are numeric.",
    "text": "2.1 First take the input variables and split into numeric and non numeric goups. State name, education levels, NAICS_NAME are all categoric variables. Min years experience and duration are numeric.\n\n# Suppose you have these columns\ncategorical_cols = [\"STATE_NAME\", \"EDUCATION_LEVELS_NAME\",\"NAICS2_NAME\"]\nnumeric_cols = [\"MIN_YEARS_EXPERIENCE\", \"DURATION\"]\n\n\n2.1.1 For the categorical columns, assign index values to each column and then one hot encode the columns as a vector.\n\nindexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\") for col in categorical_cols]\nencoders = [OneHotEncoder(inputCols=[f\"{col}_indexed\"], outputCols=[f\"{col}_encoded\"]) for col in categorical_cols]\n\n\n\n2.1.2 Next compile the one hot encoded columns with the numeric columns in a vector to be used in feature modeling.\n\nassembler_inputs = [f\"{col}_encoded\" for col in categorical_cols] + numeric_cols\nassembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n\n\n\n2.1.3 Store these data preparation steps as a pipeline for further use\n\npipeline = Pipeline(stages=indexers + encoders + [assembler])\n\n\n\n2.1.4 For polynomial square min years experience\n\nfrom pyspark.sql.functions import col, pow\n\ndf_poly = df.withColumn(\"MIN_YEARS_EXPERIENCE_SQ\", pow(col(\"MIN_YEARS_EXPERIENCE\"), 2))\n\n\n\n2.1.5 Assemble vector using min years and min years experience for polynomial features.\n\nfrom pyspark.ml.feature import VectorAssembler\n\nassembler_poly = VectorAssembler(\n    inputCols=[\"MIN_YEARS_EXPERIENCE\", \"MIN_YEARS_EXPERIENCE_SQ\"],\n    outputCol=\"features_poly\"\n)\n\ndf_poly = assembler_poly.transform(df_poly)\n\n\ndf_poly.printSchema()\n\nroot\n |-- SALARY: integer (nullable = true)\n |-- STATE_NAME: string (nullable = true)\n |-- NAICS2_NAME: string (nullable = true)\n |-- EDUCATION_LEVELS_NAME: string (nullable = true)\n |-- MIN_YEARS_EXPERIENCE: integer (nullable = false)\n |-- DURATION: integer (nullable = false)\n |-- MIN_YEARS_EXPERIENCE_SQ: double (nullable = false)\n |-- features_poly: vector (nullable = true)\n\n\n\n\n#df_poly.show()\n\n\n\n2.1.6 Now split the data for training and testing in a 70/30% split.\n\ntrain_df, test_df = df.randomSplit([0.7, 0.3], seed=42)\n\n\n\n2.1.7 Use the previusly created pipeline the prepare the training and test data for use.\n\npipeline_model = pipeline.fit(train_df)\ntrain_ready = pipeline_model.transform(train_df)\ntest_ready = pipeline_model.transform(test_df)\n\n[Stage 4:&gt;                                                          (0 + 1) / 1]                                                                                [Stage 10:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 16:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\n\n\n2.1.8 Parse target variable and features vector for modeling.\n\ntrain_ready = train_ready[\"SALARY\",\"features\"]\n#train_ready.show()\n\ntest_ready = test_ready[\"SALARY\",\"features\"]\n\n\n\n2.1.9 Confirm schema for linear regression model.\n\ntrain_ready.printSchema()\n\nroot\n |-- SALARY: integer (nullable = true)\n |-- features: vector (nullable = true)"
  },
  {
    "objectID": "assignment04-tluck15.html#build-and-fit-model",
    "href": "assignment04-tluck15.html#build-and-fit-model",
    "title": "Assignment 04",
    "section": "3.1 Build and fit model",
    "text": "3.1 Build and fit model\n\n3.1.1 Penalalize large coefficients and keep all features. Allow for intercept and standardize the data accross variables.\n\nlrm = LinearRegression(\n  featuresCol=\"features\",\n  labelCol=\"SALARY\",\n  predictionCol=\"prediction\",\n  maxIter=100,\n  regParam=0.1,\n  elasticNetParam=0.0,\n  fitIntercept=True,\n  standardization=True,\n)\n\nmodelLR = lrm.fit(train_ready)\n\n[Stage 22:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 23:&gt;                                                         (0 + 1) / 1]"
  },
  {
    "objectID": "assignment04-tluck15.html#linear-regression-model-summary-on-train-and-test-data",
    "href": "assignment04-tluck15.html#linear-regression-model-summary-on-train-and-test-data",
    "title": "Assignment 04",
    "section": "3.2 Linear Regression Model Summary on Train and test data",
    "text": "3.2 Linear Regression Model Summary on Train and test data\n\nsummary = modelLR.summary\n\n\n3.2.1 Issue existed with spillover so control with se, tvals, pvals. For features, loop through for only the number of coefficients so to avoid spillover.\n\n\n3.2.2 Linear regression model performed well. All features had statistically significant p values, moderately large t values, reasonably equivalent coefficient magnitudes with respect to salary, which is reflected in the stndard error being significantly smaller than the respective coefficient.\n\nse = summary.coefficientStandardErrors[1:]\ntvals = summary.tValues[1:]\npvals = summary.pValues[1:]\n\ncoef_df = pd.DataFrame({\n    \"Feature\": [f\"feature_{i+1}\" for i in range(len(modelLR.coefficients))],\n    \"Coefficient\": modelLR.coefficients.toArray(),\n    \"StdError\": se,\n    \"tValue\": tvals,\n    \"pValue\": pvals\n})\n\ncoef_df.head(100)\n\n\n\n\n\n\n\n\nFeature\nCoefficient\nStdError\ntValue\npValue\n\n\n\n\n0\nfeature_1\n27160.771987\n5586.524673\n3.197485\n1.388322e-03\n\n\n1\nfeature_2\n17862.826140\n5604.708248\n3.970201\n7.204534e-05\n\n\n2\nfeature_3\n22251.815915\n5628.321573\n2.900919\n3.724464e-03\n\n\n3\nfeature_4\n16327.306545\n5639.799141\n3.565083\n3.645294e-04\n\n\n4\nfeature_5\n20106.349447\n5650.125035\n3.825928\n1.306483e-04\n\n\n...\n...\n...\n...\n...\n...\n\n\n93\nfeature_94\n-13950.827368\n9974.182772\n-0.376460\n7.065783e-01\n\n\n94\nfeature_95\n-3754.885150\n10060.013381\n0.278668\n7.805023e-01\n\n\n95\nfeature_96\n2803.403880\n73.807420\n74.588198\n0.000000e+00\n\n\n96\nfeature_97\n5505.162463\n15.692949\n-6.188578\n6.180532e-10\n\n\n97\nfeature_98\n-97.117036\n36792.595281\n0.164635\n8.692328e-01\n\n\n\n\n98 rows × 5 columns"
  },
  {
    "objectID": "assignment04-tluck15.html#now-validate-and-run-the-model-using-the-test-data",
    "href": "assignment04-tluck15.html#now-validate-and-run-the-model-using-the-test-data",
    "title": "Assignment 04",
    "section": "3.3 Now validate and run the model using the test data",
    "text": "3.3 Now validate and run the model using the test data\n\ntestmodelLR = lrm.fit(test_ready)\n\n[Stage 24:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 25:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\n\ntestsummary = testmodelLR.summary\n\nt_values = summary.tValues\np_values = summary.pValues\n\nstats_df = pd.DataFrame({\n    \"t_value\": t_values,\n    \"p_value\": p_values\n})\n\ndesc_stats = stats_df.describe()\nprint(desc_stats)\n\n         t_value    p_value\ncount  99.000000  99.000000\nmean    2.496936   0.176394\nstd     7.503150   0.265126\nmin    -6.188578   0.000000\n25%     0.898590   0.003787\n50%     2.039629   0.036848\n75%     2.887619   0.279912\nmax    74.588198   0.981592\n\n\n\n3.3.1 Issue existed with spillover so control with se, tvals, pvals. For features, loop through for only the number of coefficients so to avoid spillover.\n\n\n3.3.2 The linear regression model performed okay on the test data. Accross the 99 features, the average p value was not significant, however the median value was significant at appx. .037 - the 75th percentile is not significant but the 25th is. THere were a few features that produced extreme results in all of the t value (~-6/74), p value (0/.98), and coefficient (a few spikes). R squared states ~35% of variation in salary can be explained by the features in the data. On average predictions are off by ~$35,834.\n\nse = testsummary.coefficientStandardErrors[1:]\ntvals = testsummary.tValues[1:]\npvals = testsummary.pValues[1:]\n\ntestcoef_df = pd.DataFrame({\n    \"Feature\": [f\"feature_{i+1}\" for i in range(len(testmodelLR.coefficients))],\n    \"Coefficient\": testmodelLR.coefficients.toArray(),\n    \"StdError\": se,\n    \"tValue\": tvals,\n    \"pValue\": pvals\n})\n\nprint(\"R2:\", testsummary.r2)\nprint(\"RMSE:\", testsummary.rootMeanSquaredError)\n\n#coef_df.head(100)\n# Scale coefficients to t-values range\ntestcoef_df[\"Coefficient_log\"] = np.log(np.abs(testcoef_df[\"Coefficient\"]) + 1e-8) \n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,6))\n\n# x-axis = features\nx = testcoef_df[\"Feature\"]\n\n# plot each statistic as a line\nplt.plot(x, testcoef_df[\"Coefficient_log\"], marker='o', label=\"Coefficient Scaled\")\nplt.plot(x, testcoef_df[\"tValue\"], marker='s', label=\"t-value\")\nplt.plot(x, testcoef_df[\"pValue\"], marker='^', label=\"p-value\")\n\nplt.axhline(y=2, color='red', linestyle=':', label='t-value threshold')\n\nplt.xticks(rotation=45, ha='right')  # rotate feature names for readability\nplt.xlabel(\"Features\")\nplt.ylabel(\"Values\")\nplt.title(\"Model Summary: Coefficients, t-values, p-values\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nR2: 0.35636023407578243\nRMSE: 35834.08039301742"
  },
  {
    "objectID": "assignment04-tluck15.html#combine-numeric-and-non-numeric-columns.",
    "href": "assignment04-tluck15.html#combine-numeric-and-non-numeric-columns.",
    "title": "Assignment 04",
    "section": "4.1 Combine numeric and non numeric columns.",
    "text": "4.1 Combine numeric and non numeric columns.\n\nassembler_poly = VectorAssembler(\n    inputCols=[\"MIN_YEARS_EXPERIENCE\", \"DURATION\"],\n    outputCol=\"numeric_features\"\n)\n\n#expand to polynomial\npoly_expansion = PolynomialExpansion(\n    degree=2, \n    inputCol=\"numeric_features\", \n    outputCol=\"poly_features\"\n)\n\n\n4.1.1 Use the pipeline created earlier to prepare the polynomial features.\n\nassembler_final = VectorAssembler(\n    inputCols=[\"poly_features\"] + [f\"{col}_encoded\" for col in categorical_cols],\n    outputCol=\"features\"\n)\n\npoly_pipeline = Pipeline(stages=indexers + encoders + [assembler_poly, poly_expansion, assembler_final])\n\n\n\n4.1.2 Fit model and apply same parameters as the linear regression. ~36.8% of variation in salary can be explained by the feature variables. And on average the prediction was off by $36,025.\n\npoly_model = poly_pipeline.fit(train_df)\ntrain_poly = poly_model.transform(train_df)\ntest_poly = poly_model.transform(test_df)\n\nlrm_poly = LinearRegression(\n    featuresCol=\"features\",\n    labelCol=\"SALARY\",\n    predictionCol=\"prediction\",\n    maxIter=100,\n    regParam=0.1,\n    elasticNetParam=0.0\n)\n\nmodel_poly = lrm_poly.fit(train_poly)\nsummary_poly = model_poly.summary\n\nprint(\"R2:\", summary_poly.r2)\nprint(\"RMSE:\", summary_poly.rootMeanSquaredError)\n\n[Stage 26:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 32:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 38:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 44:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 45:&gt;                                                         (0 + 1) / 1]                                                                                25/10/17 14:13:56 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n[Stage 46:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 47:&gt;                                                         (0 + 1) / 1]\n\n\nR2: 0.36831696100365496\nRMSE: 36025.51327867095\n\n\n                                                                                \n\n\n\nse = summary_poly.coefficientStandardErrors[1:]\ntvals = summary_poly.tValues[1:]\npvals = summary_poly.pValues[1:]\n\ncoef_df = pd.DataFrame({\n    \"Feature\": [f\"feature_{i+1}\" for i in range(len(model_poly.coefficients))],\n    \"Coefficient\": model_poly.coefficients.toArray(),\n    \"StdError\": se,\n    \"tValue\": tvals,\n    \"pValue\": pvals\n})\n\ncoef_df.head()\n\n\n\n\n\n\n\n\nFeature\nCoefficient\nStdError\ntValue\npValue\n\n\n\n\n0\nfeature_1\n4534.036373\n16.938884\n5.113944\n3.182049e-07\n\n\n1\nfeature_2\n86.624507\n49.042361\n-8.224292\n2.220446e-16\n\n\n2\nfeature_3\n-403.338710\n4.098090\n0.271263\n7.861914e-01\n\n\n3\nfeature_4\n1.111660\n0.982271\n6.941764\n3.982370e-12\n\n\n4\nfeature_5\n6.818695\n5553.886969\n4.923227\n8.575835e-07\n\n\n\n\n\n\n\n\n\n4.1.3 Overall the polynomial model performed worse than the linear regression model. All of the p value, t value, and coefficients experience high volatilitity and varying results.\n\n# Scale coefficients with log\ncoef_df[\"Coefficient_log\"] = np.log(np.abs(coef_df[\"Coefficient\"]) + 1e-8) \n\nplt.figure(figsize=(12,6))\n\n# x-axis = features\nx = coef_df[\"Feature\"]\n\n# plot each statistic as a line\nplt.plot(x, coef_df[\"Coefficient_log\"], marker='o', label=\"Coefficient Scaled\")\nplt.plot(x, coef_df[\"tValue\"], marker='s', label=\"t-value\")\nplt.plot(x, coef_df[\"pValue\"], marker='^', label=\"p-value\")\n\nplt.axhline(y=2, color='red', linestyle=':', label='t-value threshold')\n\nplt.xticks(rotation=45, ha='right')  # rotate feature names for readability\nplt.xlabel(\"Features\")\nplt.ylabel(\"Values\")\nplt.title(\"Model Summary: Coefficients, t-values, p-values\")\nplt.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "assignment04-tluck15.html#setup-model",
    "href": "assignment04-tluck15.html#setup-model",
    "title": "Assignment 04",
    "section": "5.1 Setup model",
    "text": "5.1 Setup model\n\n5.1.1 100 trees, 10 levels of depth to each branch, max of 18 bins for feature categorization.\n\nfrom pyspark.ml.regression import RandomForestRegressor\n\nrf = RandomForestRegressor(\n    labelCol=\"SALARY\",\n    featuresCol=\"features\",\n    numTrees=100,         \n    maxDepth=10,         \n    maxBins=18,        \n    seed=42\n)\n\n\n\n5.1.2 Like the other models, assemble the data and prepare it using the pipeline setup earlier in the project.\n\nrf_pipeline = Pipeline(stages=indexers + encoders + [assembler, rf])\n\n\n\n5.1.3 Train and then make predictions on test data.\n\nrf_model = rf_pipeline.fit(train_df)\npredictions = rf_model.transform(test_df)\n\n[Stage 48:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 54:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 60:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 66:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 67:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 68:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 70:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 72:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 74:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 76:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 78:&gt;                                                         (0 + 1) / 1]                                                                                25/10/17 14:14:52 WARN DAGScheduler: Broadcasting large task binary with size 1319.1 KiB\n[Stage 80:&gt;                                                         (0 + 1) / 1]                                                                                25/10/17 14:14:55 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n[Stage 82:&gt;                                                         (0 + 1) / 1][Stage 83:&gt;                                                         (0 + 1) / 1]                                                                                25/10/17 14:14:58 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n[Stage 84:&gt;                                                         (0 + 1) / 1]                                                                                25/10/17 14:15:02 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n[Stage 86:&gt;                                                         (0 + 1) / 1][Stage 87:&gt;                                                         (0 + 1) / 1]                                                                                25/10/17 14:15:07 WARN DAGScheduler: Broadcasting large task binary with size 7.4 MiB\n[Stage 88:&gt;                                                         (0 + 1) / 1][Stage 89:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\n\n\n5.1.4 Initial results suggest the random forest performed reasonably well. The r squared suggests ~43% of variation in salary can be explained by the features in the forest. Error is comparable to the other models as on average the prediction of salary was $25,175 off.\n\nevaluator = RegressionEvaluator(\n    labelCol=\"SALARY\",\n    predictionCol=\"prediction\",\n    metricName=\"r2\"\n)\n\nr2 = evaluator.evaluate(predictions)\nrmse = RegressionEvaluator(\n    labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"rmse\"\n).evaluate(predictions)\n\nmae = RegressionEvaluator(\n    labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"mae\"\n).evaluate(predictions)\n\nprint(f\"R²: {r2:.3f}\")\nprint(f\"RMSE: {rmse:.3f}\")\nprint(f\"MAE: {mae:.3f}\")\n\n[Stage 90:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 91:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 92:&gt;                                                         (0 + 1) / 1]\n\n\nR²: 0.429\nRMSE: 33744.372\nMAE: 25175.654\n\n\n                                                                                \n\n\n\n\n5.1.5 Feature 97 was by far the most important feature.\n\nrf_stage = rf_model.stages[-1] \n\nimportances = rf_stage.featureImportances.toArray()\n\nfeat_imp = pd.DataFrame({\n    \"Feature\": [f\"feature_{i+1}\" for i in range(len(importances))],\n    \"Importance\": importances\n}).sort_values(by=\"Importance\", ascending=False)\n\nfeat_imp.head(10)\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n96\nfeature_97\n0.521858\n\n\n54\nfeature_55\n0.083882\n\n\n76\nfeature_77\n0.069038\n\n\n80\nfeature_81\n0.045592\n\n\n97\nfeature_98\n0.031267\n\n\n55\nfeature_56\n0.029943\n\n\n83\nfeature_84\n0.025972\n\n\n52\nfeature_53\n0.016110\n\n\n77\nfeature_78\n0.013780\n\n\n51\nfeature_52\n0.013470\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(10,6))\nsns.barplot(data=feat_imp.head(15), y=\"Feature\", x=\"Importance\", palette=\"viridis\")\nplt.title(\"Top 15 Random Forest Feature Importances\")\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_3501/3261347940.py:5: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect."
  },
  {
    "objectID": "assignment04-tluck15.html#summary",
    "href": "assignment04-tluck15.html#summary",
    "title": "Assignment 04",
    "section": "6.1 Summary",
    "text": "6.1 Summary\n\n6.1.1 Overall the linear regression model was the model that produced the most consistent results on the test data. Generally speaking across all features the T-stat was consistently high (coeff/error), the p[ value was consistently at a level of significance (&lt;.05), and this was representative of all variables, meaning all of the variables seemed to contribute reasonably to the outcome in terms of the coefficient magnitude, the t-stat and the p-value. The random foorest performed better in r squared (percent variation in salary explained by the features) but the rest of the supporting data was inconsinstent, especially the expnential impact of feature 97 on the random forest. The polynomial model also had inconsistencies, which make it less consistenly effective than the linear regression.\n\n\n6.1.2 In total the linear regression accross the 99 features, the average p value was not significant, however the median value was significant at appx. .037 - the 75th percentile is not significant but the 25th is. There were a few features that produced extreme results in all of the t value (~-6/74), p value (0/.98), and coefficient (a few spikes). R squared states ~35% of variation in salary can be explained by the features in the data. On average predictions are off by ~$35,834."
  }
]