---
title: Assignment 04
author:
  - name: Norah Jones
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: '2024-11-21'
date-modified: today
date-format: long
format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2
  docx: default
execute:
  echo: true
  eval: true
  freeze: auto
---

```{python}
#| echo: false
from pyspark.sql import SparkSession
import pandas as pd
import plotly.express as px
import plotly.io as pio
import numpy as np
```
# Data Preparartion
## Load data and review
```{python}
np.random.seed(42)

pio.renderers.default = "notebook+notebook_connected+vscode"

# Initialize Spark Session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("data/lightcast_job_postings.csv")

# Show Schema and Sample Data
#print("---This is Diagnostic check, No need to print it in the final doc---")

#df.printSchema() # comment this line when rendering the submission
df.show(5)
```
### Pick salary as target variable, state name, NAICS2_NAME name, remote type name, employment type name, city name, education levels name, min years experience , duration will be indepent variables for the analysis. 

### For min years experience and duration fill na with 0. 

```{python}
df = df.select("SALARY","STATE_NAME","NAICS2_NAME", "EDUCATION_LEVELS_NAME", "MIN_YEARS_EXPERIENCE", "DURATION")
#df.show()
df = df.na.fill({"MIN_YEARS_EXPERIENCE": 0, "DURATION":0})
#df.show()
```

### Visualize the nas to understand the magnitude. Over 50% of salary is na, remove those values. 
```{python}
df_pd = df.toPandas()
#df_pd.head(5)
(df_pd.isna().sum() / len(df_pd)) * 100

```

```{python}
#| echo: false
import seaborn as sns
import matplotlib.pyplot as plt
```

### Using seaborn review a heat map of NA values. The independant variables are whole but salary is missing data.
```{python}
sns.heatmap(df_pd.isna(), cbar=False)
plt.title("Missing Data Heatmap")
plt.show()
```

### Drop all records where salary is NA.
```{python}
df = df.na.drop(subset=["SALARY"])
df_pd = df.toPandas()

sns.heatmap(df_pd.isna(), cbar=False)
plt.title("Missing Data Heatmap")
plt.show()
```
### The data is cleaned and is ready for modeling.

```{python}
(df_pd.isna().sum() / len(df_pd)) * 100
```

```{python}
#| echo: false
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
```
# Feature Engineering
## First take the input variables and split into numeric and non numeric goups. State name, education levels, NAICS_NAME are all categoric variables. Min years experience and duration are numeric.

```{python}
# Suppose you have these columns
categorical_cols = ["STATE_NAME", "EDUCATION_LEVELS_NAME","NAICS2_NAME"]
numeric_cols = ["MIN_YEARS_EXPERIENCE", "DURATION"]
```

### For the categorical columns, assign index values to each column and then one hot encode the columns as a vector.
```{python}

indexers = [StringIndexer(inputCol=col, outputCol=f"{col}_indexed") for col in categorical_cols]
encoders = [OneHotEncoder(inputCols=[f"{col}_indexed"], outputCols=[f"{col}_encoded"]) for col in categorical_cols]
```

### Next compile the one hot encoded columns with the numeric columns in a vector to be used in feature modeling.
```{python}

assembler_inputs = [f"{col}_encoded" for col in categorical_cols] + numeric_cols
assembler = VectorAssembler(inputCols=assembler_inputs, outputCol="features")
```


### Store these data preparation steps as a pipeline for further use
```{python}

pipeline = Pipeline(stages=indexers + encoders + [assembler])
```

### For polynomial square min years experience
```{python}
from pyspark.sql.functions import col, pow

df_poly = df.withColumn("MIN_YEARS_EXPERIENCE_SQ", pow(col("MIN_YEARS_EXPERIENCE"), 2))

```
### Assemble vector using min years and min years experience for polynomial features.
```{python}
from pyspark.ml.feature import VectorAssembler

assembler_poly = VectorAssembler(
    inputCols=["MIN_YEARS_EXPERIENCE", "MIN_YEARS_EXPERIENCE_SQ"],
    outputCol="features_poly"
)

df_poly = assembler_poly.transform(df_poly)
```

```{python}
df_poly.printSchema()
```
```{python}
#df_poly.show()
```
### Now split the data for training and testing in a 70/30% split. 
```{python}
train_df, test_df = df.randomSplit([0.7, 0.3], seed=42)
```

### Use the previusly created pipeline the prepare the training and test data for use. 
```{python}
pipeline_model = pipeline.fit(train_df)
train_ready = pipeline_model.transform(train_df)
test_ready = pipeline_model.transform(test_df)
```

### Parse target variable and features vector for modeling.
```{python}
train_ready = train_ready["SALARY","features"]
#train_ready.show()

test_ready = test_ready["SALARY","features"]
```


```{python}
#| echo: false
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator 
```

### Confirm schema for linear regression model.
```{python}
train_ready.printSchema()
```

```{python}
lrm = LinearRegression(
  featuresCol="features",
  labelCol="SALARY",
  predictionCol="prediction",
  maxIter=100,
  regParam=0.0,
  elasticNetParam=0.0,
  fitIntercept=True,
  standardization=True,
)

modelLR = lrm.fit(train_ready)
```

```{python}
summary = modelLR.summary
```

```{python}
# Drop the intercept-related first element from the summary stats
se = summary.coefficientStandardErrors[1:]
tvals = summary.tValues[1:]
pvals = summary.pValues[1:]

# Now build the DataFrame — they should all match
coef_df = pd.DataFrame({
    "Feature": [f"feature_{i+1}" for i in range(len(modelLR.coefficients))],
    "Coefficient": modelLR.coefficients.toArray(),
    "StdError": se,
    "tValue": tvals,
    "pValue": pvals
})

coef_df.head()
```

# Polynomial Regression

```{python}
from pyspark.ml.feature import PolynomialExpansion, VectorAssembler

```

```{python}
assembler_poly = VectorAssembler(
    inputCols=["MIN_YEARS_EXPERIENCE", "DURATION"],
    outputCol="numeric_features"
)

#expand to polynomial
poly_expansion = PolynomialExpansion(
    degree=2, 
    inputCol="numeric_features", 
    outputCol="poly_features"
)
```

```{python}
assembler_final = VectorAssembler(
    inputCols=["poly_features"] + [f"{col}_encoded" for col in categorical_cols],
    outputCol="features"
)

poly_pipeline = Pipeline(stages=indexers + encoders + [assembler_poly, poly_expansion, assembler_final])

```


```{python}
poly_model = poly_pipeline.fit(train_df)
train_poly = poly_model.transform(train_df)
test_poly = poly_model.transform(test_df)

lrm_poly = LinearRegression(
    featuresCol="features",
    labelCol="SALARY",
    predictionCol="prediction",
    maxIter=100,
    regParam=0.0,
    elasticNetParam=0.0
)

model_poly = lrm_poly.fit(train_poly)
summary_poly = model_poly.summary

print("R2:", summary_poly.r2)
print("RMSE:", summary_poly.rootMeanSquaredError)
```

```{python}
se = summary_poly.coefficientStandardErrors[1:]
tvals = summary_poly.tValues[1:]
pvals = summary_poly.pValues[1:]

coef_df = pd.DataFrame({
    "Feature": [f"feature_{i+1}" for i in range(len(model_poly.coefficients))],
    "Coefficient": model_poly.coefficients.toArray(),
    "StdError": se,
    "tValue": tvals,
    "pValue": pvals
})

coef_df.head()
```

# Random Forest
```{python}
from pyspark.ml.regression import RandomForestRegressor

rf = RandomForestRegressor(
    labelCol="SALARY",
    featuresCol="features",
    numTrees=100,           # number of trees (more trees = more stability)
    maxDepth=10,            # depth of each tree
    maxBins=32,             # controls how continuous features are binned
    seed=42
)
```

```{python}
rf_pipeline = Pipeline(stages=indexers + encoders + [assembler, rf])

```

```{python}
rf_model = rf_pipeline.fit(train_df)
predictions = rf_model.transform(test_df)

```

```{python}
evaluator = RegressionEvaluator(
    labelCol="SALARY",
    predictionCol="prediction",
    metricName="r2"
)

r2 = evaluator.evaluate(predictions)
rmse = RegressionEvaluator(
    labelCol="SALARY", predictionCol="prediction", metricName="rmse"
).evaluate(predictions)

mae = RegressionEvaluator(
    labelCol="SALARY", predictionCol="prediction", metricName="mae"
).evaluate(predictions)

print(f"R²: {r2:.3f}")
print(f"RMSE: {rmse:.3f}")
print(f"MAE: {mae:.3f}")


```


```{python}
rf_stage = rf_model.stages[-1] 

importances = rf_stage.featureImportances.toArray()

feat_imp = pd.DataFrame({
    "Feature": [f"feature_{i+1}" for i in range(len(importances))],
    "Importance": importances
}).sort_values(by="Importance", ascending=False)

feat_imp.head(10)
```

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10,6))
sns.barplot(data=feat_imp.head(15), y="Feature", x="Importance", palette="viridis")
plt.title("Top 15 Random Forest Feature Importances")
plt.tight_layout()
plt.show()
```

# Model Comparisons