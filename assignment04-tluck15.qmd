---
title: Assignment 04
author:
  - name: Norah Jones
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: '2024-11-21'
date-modified: today
date-format: long
format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2
  docx: default
execute:
  echo: true
  eval: true
  freeze: auto
---

```{python}
from pyspark.sql import SparkSession
import pandas as pd
import plotly.express as px
import plotly.io as pio
import numpy as np
```
# Data Preparartion
## Load data and review
```{python}
np.random.seed(42)

pio.renderers.default = "notebook+notebook_connected+vscode"

# Initialize Spark Session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("data/lightcast_job_postings.csv")

# Show Schema and Sample Data
print("---This is Diagnostic check, No need to print it in the final doc---")

#df.printSchema() # comment this line when rendering the submission
df.show(5)
```
### Pick salary as target variable, state name, posted, certifications name, remote type name, employment type name, city name, education levels name swill be indepent variables for the analysis.

```{python}
df = df.select("SALARY","STATE_NAME","NAICS2_NAME", "EDUCATION_LEVELS_NAME", "MIN_YEARS_EXPERIENCE", "DURATION")
df.show()
df = df.na.fill({"MIN_YEARS_EXPERIENCE": 0, "DURATION":0})
df.show()
```

```{python}
df_pd = df.toPandas()
#df_pd.head(5)
(df_pd.isna().sum() / len(df_pd)) * 100

```
### Visualize the nas to understand the magnitude. Over 50% of salary is na, remove those values.
```{python}
import seaborn as sns
import matplotlib.pyplot as plt
```

```{python}
sns.heatmap(df_pd.isna(), cbar=False)
plt.title("Missing Data Heatmap")
plt.show()
```
```{python}
df = df.na.drop(subset=["SALARY"])
df_pd = df.toPandas()

sns.heatmap(df_pd.isna(), cbar=False)
plt.title("Missing Data Heatmap")
plt.show()
```

```{python}
(df_pd.isna().sum() / len(df_pd)) * 100
```

```{python}
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
```

```{python}
# Suppose you have these columns
categorical_cols = ["STATE_NAME", "EDUCATION_LEVELS_NAME","NAICS2_NAME"]
numeric_cols = ["MIN_YEARS_EXPERIENCE", "DURATION"]
```

```{python}
# Step 1: Index and encode categorical variables
indexers = [StringIndexer(inputCol=col, outputCol=f"{col}_indexed") for col in categorical_cols]
encoders = [OneHotEncoder(inputCols=[f"{col}_indexed"], outputCols=[f"{col}_encoded"]) for col in categorical_cols]
```

```{python}
# Step 2: Assemble all features into one vector
assembler_inputs = [f"{col}_encoded" for col in categorical_cols] + numeric_cols
assembler = VectorAssembler(inputCols=assembler_inputs, outputCol="features")
```


```{python}
# Step 3: Define the pipeline
pipeline = Pipeline(stages=indexers + encoders + [assembler])
```

```{python}
from pyspark.sql.functions import col, pow

df_poly = df.withColumn("MIN_YEARS_EXPERIENCE_SQ", pow(col("MIN_YEARS_EXPERIENCE"), 2))

```

```{python}
from pyspark.ml.feature import VectorAssembler

assembler_poly = VectorAssembler(
    inputCols=["MIN_YEARS_EXPERIENCE", "MIN_YEARS_EXPERIENCE_SQ"],
    outputCol="features_poly"
)

df_poly = assembler_poly.transform(df_poly)
```

```{python}
df_poly.printSchema()
```
```{python}
df_poly.show()
```
```{python}
# Step 4: Split data
train_df, test_df = df.randomSplit([0.7, 0.3], seed=42)
```

```{python}
# Step 5: Fit and transform
pipeline_model = pipeline.fit(train_df)
train_ready = pipeline_model.transform(train_df)
test_ready = pipeline_model.transform(test_df)
```

```{python}
train_ready = train_ready["SALARY","features"]
train_ready.show()

test_ready = test_ready["SALARY","features"]

#train_ready.summary()
```


```{python}
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator 
```

```{python}
train_ready.printSchema()
```

```{python}
lrm = LinearRegression(
  featuresCol="features",
  labelCol="SALARY",
  predictionCol="prediction",
  maxIter=100,
  regParam=0.0,
  elasticNetParam=0.0,
  fitIntercept=True,
  standardization=True,
)

modelLR = lrm.fit(train_ready)
```

```{python}
summary = modelLR.summary
```

```{python}
# Drop the intercept-related first element from the summary stats
se = summary.coefficientStandardErrors[1:]
tvals = summary.tValues[1:]
pvals = summary.pValues[1:]

# Now build the DataFrame â€” they should all match
coef_df = pd.DataFrame({
    "Feature": [f"feature_{i+1}" for i in range(len(modelLR.coefficients))],
    "Coefficient": modelLR.coefficients.toArray(),
    "StdError": se,
    "tValue": tvals,
    "pValue": pvals
})

coef_df.head()
```

# Polynomial Regression

```{python}
from pyspark.ml.feature import PolynomialExpansion, VectorAssembler

```

```{python}
assembler_poly = VectorAssembler(
    inputCols=["MIN_YEARS_EXPERIENCE", "DURATION"],
    outputCol="numeric_features"
)

#expand to polynomial
poly_expansion = PolynomialExpansion(
    degree=2, 
    inputCol="numeric_features", 
    outputCol="poly_features"
)
```

```{python}
assembler_final = VectorAssembler(
    inputCols=["poly_features"] + [f"{col}_encoded" for col in categorical_cols],
    outputCol="features"
)

poly_pipeline = Pipeline(stages=indexers + encoders + [assembler_poly, poly_expansion, assembler_final])

```


```{python}
poly_model = poly_pipeline.fit(train_df)
train_poly = poly_model.transform(train_df)
test_poly = poly_model.transform(test_df)

lrm_poly = LinearRegression(
    featuresCol="features",
    labelCol="SALARY",
    predictionCol="prediction",
    maxIter=100,
    regParam=0.0,
    elasticNetParam=0.0
)

model_poly = lrm_poly.fit(train_poly)
summary_poly = model_poly.summary

print("R2:", summary_poly.r2)
print("RMSE:", summary_poly.rootMeanSquaredError)
```

```{python}
se = summary_poly.coefficientStandardErrors[1:]
tvals = summary_poly.tValues[1:]
pvals = summary_poly.pValues[1:]

coef_df = pd.DataFrame({
    "Feature": [f"feature_{i+1}" for i in range(len(model_poly.coefficients))],
    "Coefficient": model_poly.coefficients.toArray(),
    "StdError": se,
    "tValue": tvals,
    "pValue": pvals
})

coef_df.head()
```