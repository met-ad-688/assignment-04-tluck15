---
title: Assignment 04
author:
  - name: Norah Jones
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: '2024-11-21'
date-modified: today
date-format: long
format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2
  docx: default
execute:
  echo: true
  eval: true
  freeze: auto
---

```{python}
from pyspark.sql import SparkSession
import pandas as pd
import plotly.express as px
import plotly.io as pio
import numpy as np
```
# Data Preparartion
## Load data and review
```{python}
np.random.seed(42)

pio.renderers.default = "notebook+notebook_connected+vscode"

# Initialize Spark Session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("data/lightcast_job_postings.csv")

# Show Schema and Sample Data
print("---This is Diagnostic check, No need to print it in the final doc---")

#df.printSchema() # comment this line when rendering the submission
df.show(5)
```
### Pick salary as target variable, state name, posted, certifications name, remote type name, employment type name, city name, education levels name swill be indepent variables for the analysis.

```{python}
df = df.select("SALARY","STATE_NAME","POSTED", "EDUCATION_LEVELS_NAME", "MIN_YEARS_EXPERIENCE", "DURATION")
df.show()
df = df.na.fill({"MIN_YEARS_EXPERIENCE": 0, "DURATION":0})
df.show()
```

```{python}
df_pd = df.toPandas()
#df_pd.head(5)
(df_pd.isna().sum() / len(df_pd)) * 100

```
### Visualize the nas to understand the magnitude. Over 50% of salary is na, remove those values.
```{python}
import seaborn as sns
import matplotlib.pyplot as plt
```

```{python}
sns.heatmap(df_pd.isna(), cbar=False)
plt.title("Missing Data Heatmap")
plt.show()
```
```{python}
df = df.na.drop(subset=["SALARY"])
df_pd = df.toPandas()

sns.heatmap(df_pd.isna(), cbar=False)
plt.title("Missing Data Heatmap")
plt.show()
```

```{python}
(df_pd.isna().sum() / len(df_pd)) * 100
```

```{python}
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
```

```{python}
# Suppose you have these columns
categorical_cols = ["STATE_NAME", "EDUCATION_LEVELS_NAME","POSTED"]
numeric_cols = ["SALARY", "MIN_YEARS_EXPERIENCE", "DURATION"]
```

```{python}
# Step 1: Index and encode categorical variables
indexers = [StringIndexer(inputCol=col, outputCol=f"{col}_indexed") for col in categorical_cols]
encoders = [OneHotEncoder(inputCols=[f"{col}_indexed"], outputCols=[f"{col}_encoded"]) for col in categorical_cols]
```

```{python}
# Step 2: Assemble all features into one vector
assembler_inputs = [f"{col}_encoded" for col in categorical_cols] + numeric_cols
assembler = VectorAssembler(inputCols=assembler_inputs, outputCol="features")
```


```{python}
# Step 3: Define the pipeline
pipeline = Pipeline(stages=indexers + encoders + [assembler])
```

```{python}
from pyspark.sql.functions import col, pow

df_poly = df.withColumn("MIN_YEARS_EXPERIENCE_SQ", pow(col("MIN_YEARS_EXPERIENCE"), 2))

```

```{python}
from pyspark.ml.feature import VectorAssembler

assembler_poly = VectorAssembler(
    inputCols=["MIN_YEARS_EXPERIENCE", "MIN_YEARS_EXPERIENCE_SQ"],
    outputCol="features_poly"
)

df_poly = assembler_poly.transform(df_poly)
```

```{python}
df_poly.printSchema()
```
```{python}
df_poly.show()
```
```{python}
# Step 4: Split data
train_df, test_df = df.randomSplit([0.7, 0.3], seed=42)
```

```{python}
# Step 5: Fit and transform
pipeline_model = pipeline.fit(train_df)
train_ready = pipeline_model.transform(train_df)
test_ready = pipeline_model.transform(test_df)
```

```{python}
train_ready = train_ready["SALARY","features"]
train_ready.show()
```